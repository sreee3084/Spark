CCA 175

https://github.com/smartlin5228/CCA175

file formats:

textfile:

csv,tsv,json...etc
convenient format to use to exchange with other applications or scripts that produce or read delimited files.
human readable and parsable
data stores is bulky and not as efficient for query
do not support block compression.

sequencefile:
 data structure for binary key value pairs.
 row based.
 used to transfer data between map and reduce jobs.
 can be used as archive to pack small files in hadoop.
 support splitting even when the data is compressed.

avro:
widely used as serialization platform.
row based,offers compact and fast binary format.
support schema evolution.

parquet:
column oriented binay file format.
efficient when specific columns need to be queried.


orc:

column oriented binay file format.
having light weight indexing.
it comes with basic statics on columns (min,max,,sum and count)



how to choose for write and read:

Write:

orc and parquet takes more time to write as it needs some additional parsing.

use cases:
	avro-event data can change over a time.
	sequencefile-Datasets shared between MR jobs.
	Text-Adding larger amount of data to HDFS quickly.

	
Read:
Column specific, or few groups of columns then use 	columnor format like orc or parquet.
compression of the file regardless the format increase the query speed times.

orc or parquet preferred more for quering

Can we use ORC fileformat in Impala?

in base version is not supported but yes from impala version 2.12.0 


Q1:

You have been given below code snippet.
val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
Operation_xyz
Write a correct code snippet for Operation_xyz which will produce below output.
scalaxollection.Map[lnt,Long] = Map(5 -> 1, 8 -> 1, 3 -> 1, 6 -> 1, 1 -> S, 2 -> 3, 4 -> 2, 7 ->
1)

sol:

val b = sc.parallelize(List(1,2,3,4,5,6,7,8,2,4,2,1,1,1,1,1))
val c=b.map(b=>(b,1)).countByKey

-------
Q2

: You have been given MySQL DB with following details. You have
been given following product.csv file
product.csv
productID,productCode,name,quantity,price
1001,PEN,Pen Red,5000,1.23
1002,PEN,Pen Blue,8000,1.25
1003,PEN,Pen Black,2000,1.25
1004,PEC,Pencil 2B,10000,0.48
1005,PEC,Pencil 2H,8000,0.49
1006,PEC,Pencil HB,0,9999.99
Now accomplish following activities.
1. Create a Hive ORC table using SparkSql
2. Load this data in Hive table.
3. Create a Hive parquet table using SparkSQL and load data in it.

****

create csv file with  the above content and move the file to hdfs location(/user/cloudera/practice/Q2)

create folder in hdfs and the use vi products.csv in local then paste the content(without header) using shift insert and then :wq! then move file into created hdfs location	

hdfs dfs -put products.csv /user/cloudera/practice/Q2/

now in spark-shell

val prd=sc.textFile("/user/cloudera/practice/Q2/products.csv")
productID,productCode,name,quantity,price
val pdf=prd.map(p=>(p.split(",")(0).toInt,p.split(",")(1).toString,p.split(",")(2).toString,p.split(",")(3).toInt,p.split(",")(4).toFloat)).toDF("productid","code","name","quantity","price")
pdf.write.mode("overwrite").format("orc").saveAsTable("product_orc_table") 

pdf.write.mode("overwrite").format("parquet").saveAsTable("product_par_table")


in hive:

create external table prd_orc(product_id int,code string,name string,quantity int,price float) stored as orc  location '/user/hive/warehouse/prd_orc_table' 

--------------------
Q3:

user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Now accomplish following activities.
1. Import departments table from mysql to hdfs as textfile in departments_text directory.
2. Import departments table from mysql to hdfs as sequncefile in departments_sequence
directory.
3. Import departments table from mysql to hdfs as avro file in departments avro directory.
4. Import departments table from mysql to hdfs as parquet file in departments_parquet
directory.


******

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--as-textfile \
--target-dir /user/cloudera/practice/Q3/departments_text

hdfs dfs -ls /user/cloudera/practice/Q3/departments_text/par*

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--as-sequencefile \
--target-dir /user/cloudera/practice/Q3/departments_sequence

hdfs dfs -cat /user/cloudera/practice/Q3/departments_sequence/par*

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--as-avrodatafile \
--target-dir /user/cloudera/practice/Q3/departments_avro

hdfs dfs -cat /user/cloudera/practice/Q3/departments_avro/par*

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--as-parquetfile \
--target-dir /user/cloudera/practice/Q3/departments_parquet

hdfs dfs -cat /user/cloudera/practice/Q3/departments_parquet/*par*

-------------

Q4:

You have been given below code snippet.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) 
val b =a.keyBy(_.length)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)),
(3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle}}}


*****
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) 
val b=a.keyBy(_.length)
val c=b.groupByKey
c.collect

----------
Q5:

val a = sc.parallelize(1 to 10, 3)
operation1
b.collect
Output 1
Array[lnt] = Array(2, 4, 6, 8,10)
operation2
Output 2
Array[lnt] = Array(1,2, 3)
Write a correct code snippet for operation1 and operation2 which will produce desired
output, shown above.

***

val a = sc.parallelize(1 to 10, 3)
val b=a.filter(p=>(p%2==0))
b.collect
val c=a.filter(p=>(p%2!=0))
c.collect

----------

Q6:

You have been given below code snippet.
val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)
val b = sc.parallelize(1 to a.count.tolnt, 2)
val c = a.zip(b)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2>, (ant,5))

***

val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)
val l=a.toArray
val  res=a.map(p=>(p,l.indexOf(p)+1)).sortByKey(false).collect

---

Q7:

You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.categories
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. Connect MySQL DB and check the content of the tables.
2. Copy "retaildb.categories" table to hdfs, without specifying directory name.
3. Copy "retaildb.categories" table to hdfs, in a directory name "categories_target".
4. Copy "retaildb.categories" table to hdfs, in a warehouse directory name
"categories_warehouse".



*****
sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table categories 


sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--target-dir categories_target


sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--warehouse-dir categories_warehouse



-----
Q8:

val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val b = a.map(x => (x.length, x))
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String}] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))


****
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val b = a.map(x => (x.length, x))
b.reduceByKey(_+_) or b.reduceByKey((x,y)=>x+y)

---

Q9:

You have been given below patient data in csv format,
patientID,name,dateOfBirth,lastVisitDate
1001,Ah Teck,1991-12-31,2012-01-20
1002,Kumar,2011-10-29,2012-09-20
1003,Ali,2011-01-30,2012-10-21
Accomplish following activities.
1. Find all the patients whose lastVisitDate between current time and '2012-09-15'
2. Find all the patients who born in 2011
3. Find all the patients age
4. List patients whose last visited more than 60 days ago
5. Select patients 18 years old or younger


***

hdfs dfs -mkdir /user/cloudera/practice/Q8

vi patients.csv paste above content along with headers and then copy to hdfs using -put command

in spark-sehll:

val data=sc.textFile("/user/cloudera/practice/Q8/patients.csv")
val f=data.first
val data_filter=data.filter(p=>(p!=f))
val df=data_filter.map(p=>(p.split(",")(0).toInt,p.split(",")(1),p.split(",")(2),p.split(",")(3))).toDF("patientid","name","dob","vdate")
df.registerTempTable("patients")


sqlContext.sql("select * from patients where to_date(cast(unix_timestamp(vdate,'yyyy-MM-dd')as timestamp)) between '2012-09-15' and current_date").show

sqlContext.sql("select * from patients where year(cast(unix_timestamp(dob,'yyyy-MM-dd')as timestamp)) =2011").show

sqlContext.sql("select name,datediff(current_date,to_date(cast(unix_timestamp(dob,'yyyy-MM-dd') as timestamp)))/365 as age from patients ").show
sqlContext.sql("select * from patients where datediff(current_date,to_date(cast(unix_timestamp(vdate,'yyyy-MM-dd') as timestamp)))>60 ").show
sqlContext.sql("SELECT * FROM patients WHERE (datediff(current_date,TO_DATE(CAST(UNIX_TIMESTAMP(dob, 'yyyy-MM-dd') AS TIMESTAMP)))/365)<=18").show

				or
sqlContext.sql("SELECT * FROM patients WHERE TO_DATE(CAST(UNIX_TIMESTAMP(dob, 'yyyy-MM-dd') AS TIMESTAMP))>date_sub(current_date,18*365)").show


------

Q10:

You have been given below Python code snippet, with intermediate
output.
We want to take a list of records about people and then we want to sum up their ages and
count them.
So for this example the type in the RDD will be a Dictionary in the format of {name: NAME,
age:AGE, gender:GENDER}.
The result type will be a tuple that looks like so (Sum of Ages, Count)
people = []
people.append({'name':'Amit', 'age':45,'gender':'M'})
people.append({'name':'Ganga', 'age':43,'gender':'F'})
people.append({'name':'John', 'age':28,'gender':'M'})
people.append({'name':'Lolita', 'age':33,'gender':'F'})
people.append({'name':'Dont Know', 'age':18,'gender':'T'})
peopleRdd=sc.parallelize(people) //Create an RDD
peopleRdd.aggregate((0,0), seqOp, combOp) //Output of above line : 167, 5)
Now define two operation seqOp and combOp , such that
seqOp : Sum the age of all people as well count them, in each partition. combOp :
Combine results from all partitions.


python coding

-----
Q11

user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. In mysql departments table please insert following record. Insert into departments
values(9999, '"Data Science"1);
2. Now there is a downstream system which will process dumps of this file. However,
system is designed the way that it can process only files if fields are enlcosed in(') single
quote and separate of the field should be (-} and line needs to be terminated by : (colon).
3. If data itself contains the " (double quote } than it should be escaped by \.
4. Please import the departments table in a directory called departments_enclosedby and
file should be able to process by downstream system.

****

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table departments_rep \
--fields-terminated-by '-' \
--lines-terminated-by ':' \
--enclosed-by "'" \
--escaped-by '\\' \
--target-dir departments_enclosedby -m 1


---
Q12

user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Create a table in retailedb with following definition.
CREATE table departments_new (department_id int(11), department_name varchar(45),
created_date T1MESTAMP DEFAULT NOW());
2. Now isert records from departments table to departments_new
3. Now import data from departments_new table to hdfs.
4. Insert following 5 records in departmentsnew table. Insert into departments_new
values(110, "Civil" , null); Insert into departments_new values(111, "Mechanical" , null);
Insert into departments_new values(112, "Automobile" , null); Insert into departments_new
values(113, "Pharma" , null);
Insert into departments_new values(116, "Social sreekanth" , null);
5. Now do the incremental import based on created_date column.


****

first do normal import using below command 

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments_new \
--target-dir departments_new \
-m 1

then insert the extra rows into the mysql table and then use the below command 

sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table departments_new \
--incremental append \
--check-column created_date \
--target-dir departments_new \
--last-value '2018-08-26 09:30:16' \
-m 1


----


Q13

You have given three files as below.
spark3/sparkdir1/file1.txt
spark3/sparkd ir2ffile2.txt
spark3/sparkd ir3Zfile3.txt
Each file contain some text.
spark3/sparkdir1/file1.txt
Apache Hadoop is an open-source software framework written in Java for distributed
storage and distributed processing of very large data sets on computer clusters built from
commodity hardware. All the modules in Hadoop are designed with a fundamental
assumption that hardware failures are common and should be automatically handled by the
framework
spark3/sparkdir2/file2.txt
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File
System (HDFS) and a processing part called MapReduce. Hadoop splits files into large
blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers
packaged code for nodes to process in parallel based on the data that needs to be
processed.
spark3/sparkdir3/file3.txt
his approach takes advantage of data locality nodes manipulating the data they have
access to to allow the dataset to be processed faster and more efficiently than it would be
in a more conventional supercomputer architecture that relies on a parallel file system
where computation and data are distributed via high-speed networking
Now write a Spark code in scala which will load all these three files from hdfs and do the
word count by filtering following words. And result should be sorted by word count in
reverse order.
Filter words ("a","the","an", "as", "a","with","this","these","is","are","in", "for",
"to","and","The","of")
Also please make sure you load all three files as a Single RDD (All three files must be
loaded using single API call).
You have also been given following codec
import org.apache.hadoop.io.compress.GzipCodec
Please use above codec to compress file, while saving in hdfs.



create files and move into /user/cloudera/practice/Q13 by creating folder


val data=sc.textFile("/user/cloudera/practice/Q13/file1,/user/cloudera/practice/Q13/file2,/user/cloudera/practice/Q13/file3")
val fdata=sc.parallelize(List("a","the","an", "as", "a","with","this","these","is","are","in", "for","to","and","The","of"))
val mapdata=data.flatMap(p=>(p.split(" "))).map(p=>p.trim)
val final_data=mapdata.subtract(fdata)
val wc=final_data.map(p=>(p,1)).reduceByKey(_+_)
val wc_rev=wc.map(p=>p.swap)
val final_res=wc_rev.sortByKey(false)
final_res.saveAsTextFile("/user/cloudera/practice/Q13/res")
final_res.coalesce(1,true).saveAsTextFile("/user/cloudera/practice/Q13/res",classOf[org.apache.hadoop.io.compress.GzipCodec])

extra for knowledge:

if want to change the output format

final_res.map(p=>p._1+"\t"+p._2).coalesce(1,true).saveAsTextFile("/user/cloudera/practice/Q13/res_1")


------

Q14:

user=retail_dba
password=cloudera
database=retail_db
table=retail_db.orders
table=retail_db.order_items
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of products table : (product_id | product categoryid | product_name |
product_description | product_prtce | product_image )
Please accomplish following activities.
1. Copy "retaildb.products" table to hdfs in a directory p93_products
2. Filter out all the empty prices
3. Sort all the products based on price in both ascending as well as descending order.
4. Sort all the products based on price as well as product_id in descending order.
5. Use the below functions to do data ordering or ranking and fetch top 10 elements top()
takeOrdered() sortByKey()


sqoop import --connect jdbc:mysql://quickstart:3306/retail_db \
--username retail_dba \
--password cloudera \
--table products \
--target-dir p93_products \
-m 1


val data=sc.textFile("/user/cloudera/p93_products/part-m-00000")
val fdata=data.filter(p=>(p.split(",")(4).length>0))
val mapdata=fdata.map(p=>(p.split(",")(4).toFloat,p.split(",")(2)))
val p_asc=mapdata.sortByKey()
val p_dsc=mapdata.sortByKey(false)
val pid_price=fdata.map(p=>((p.split(",")(4).toFloat,p.split(",")(0).toInt),p.split(",")(2)))
val res=pid_price.sortByKey(false).take(10)

----for knowledge
res.map(p=>p._1._1+"\t"+p._1._2+"\t"+p._2).take(10)


-------------

Q15:

user=retail_dba
password=cloudera
database=retail_db
table=retail_db.products
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of products table : (product_id | product_category_id | product_name |
product_description | product_price | product_image )
Please accomplish following activities.
1. Copy "retaildb.products" table to hdfs in a directory p93_products
2. Now sort the products data sorted by product price per category, use productcategoryid
colunm to group by category


***
val data=sc.textFile("/user/cloudera/p93_products/part-m-00000")
val fdata=data.filter(p=>(p.split(",")(4).length>0))
val pid_price=fdata.map(p=>(p.split(",")(4).toFloat,p.split(",")(0).toInt,p.split(",")(2))).toDF("price","pid","name")
pid_price.registerTempTable("products")
sqlContext.sql("select top 10 pid,sum(price) as pprice group by pid order by pprice desc").show

--------
Q16:

You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Create a database named hadoopexam and then create a table named departments in
it, with following fields. department_id int,
department_name string
e.g. location should be
hdfs://quickstart.cloudera:8020/user/hive/warehouse/hadoopexam.db/departments
2. Please import data in existing table created above from retaidb.departments into hive
table hadoopexam.departments.
3. Please import data in a non-existing table, means while importing create hive table
named hadoopexam.departments_new



*****

in hive create database hadoopexam;
use hadoopexam;
create table departments(department_id int,department_name string)

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--fields-terminated-by '\001' \
--hive-table hadoopexam.departments


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--fields-terminated-by '\001' \
--hive-table hadoopexam.departments_new \
--create-hive-table


----
Q17

You need to implement near real time solutions for collecting
information when submitted in file with below
Data
echo "IBM,100,20160104" >> /tmp/spooldir2/.bb.txt
echo "IBM,103,20160105" >> /tmp/spooldir2/.bb.txt
mv /tmp/spooldir2/.bb.txt /tmp/spooldir2/bb.txt
After few mins
echo "IBM,100.2,20160104" >> /tmp/spooldir2/.dr.txt
echo "IBM,103.1,20160105" >> /tmp/spooldir2/.dr.txt
mv /tmp/spooldir2/.dr.txt /tmp/spooldir2/dr.txt
You have been given below directory location (if not available than create it) /tmp/spooldir2
As soon as file committed in this directory that needs to be available in hdfs in
/tmp/flume/primary as well as /tmp/flume/secondary location.
However, note that/tmp/flume/secondary is optional, if transaction failed which writes in
this directory need not to be rollback.
Write a flume configuration file named flumeS.conf and use it to load data in hdfs with
following additional properties .
1. Spool /tmp/spooldir2 directory
2. File prefix in hdfs sholuld be events
3. File suffix should be .log
4. If file is not committed and in use than it should have _ as prefix.
5. Data should be written as text to hdfs

****flume


-----
Q18:
 You have to run your Spark application on yarn with each executor
Maximum heap size to be 512MB and Number of processor cores to allocate on each
executor will be 1 and Your main application required three values as input arguments V1
V2 V3.
Please replace XXX, YYY, ZZZ
./bin/spark-submit -class com.hadoopexam.MyTask --master yarn-cluster--num-executors 3
--driver-memory 512m XXX YYY lib/hadoopexam.jarZZZ


****

xxx: --executor-memory 512m
yyy: --executor-cores 1
zzz: V1 V2 V3


---
Q19:

You have been given below code snippet (calculating an average
score}, with intermediate output.
type ScoreCollector = (Int, Double)
type PersonScores = (String, (Int, Double))
val initialScores = Array(("Fred", 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0),
("Wilma", 95.0), ("Wilma", 98.0))
val wilmaAndFredScores = sc.parallelize(initialScores).cache()
val scores = wilmaAndFredScores.combineByKey(createScoreCombiner, scoreCombiner,
scoreMerger)
val averagingFunction = (personScore: PersonScores) => { val (name, (numberScores,
totalScore)) = personScore (name, totalScore / numberScores)
val averageScores = scores.collectAsMap().map(averagingFunction)
Expected output: averageScores: scala.collection.Map[String,Double] = Map(Fred ->
91.33333333333333, Wilma -> 95.33333333333333)
Define all three required function , which are input for combineByKey method, e.g.
(createScoreCombiner, scoreCombiner, scoreMerger). And help us producing required
results.

********
val initialScores = Array(("Fred", 88.0), ("Fred", 95.0), ("Fred", 91.0), ("Wilma", 93.0),
("Wilma", 95.0), ("Wilma", 98.0))
val wilmaAndFredScores = sc.parallelize(initialScores).cache()

val df=wilmaAndFredScores.map(p=>(p._1,p._2.toFloat)).toDF("name","score")
  

df.registerTempTable("scores")

val res=sqlContext.sql("select name,avg(score) from scores group by name")
val res1=res.map(p=>(p(0)+","+p(1)))
val re2=res1.map(p=>(p.split(",")(0),p.split(",")(1).toDouble))

------
Q20:

n Continuation of previous question, please accomplish following
activities.
1. Select all the records with quantity >= 5000 and name starts with 'Pen'
2. Select all the records with quantity >= 5000, price is less than 1.24 and name starts with
'Pen'
3. Select all the records witch does not have quantity >= 5000 and name does not starts
with 'Pen'
4. Select all the products which name is 'Pen Red', 'Pen Black'
5. Select all the products which has price BETWEEN 1.0 AND 2.0 AND quantity
BETWEEN 1000 AND 2000.



***** from products data write the queries




------
Q21:
You have been given belwo list in scala (name,sex,cost) for each
work done.
List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female",
2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))
Now write a Spark program to load this list as an RDD and do the sum of cost for
combination of name and sex (as key)


*****

val l=List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female",
2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))

val rdd=sc.parallelize(l)

val m_rdd=rdd.map(p=>((p._1,p._2),p._3))
val res=m_rdd.reduceByKey((x,y)=>x+y)
res.collect


----
Q22:

You have been given below code snippet.
val aul = sc.parallelize(List (("a" , Array(1,2)), ("b" , Array(1,2))))
val au2 = sc.parallelize(List (("a" , Array(3)), ("b" , Array(2))))
Apply the Spark method, which will generate below output.
Array[(String, Array[lnt])] = Array((a,Array(1, 2)), (b,Array(1, 2)), (a(Array(3)), (b,Array(2)))

****

val aul = sc.parallelize(List (("a" , Array(1,2)), ("b" , Array(1,2))))
val au2 = sc.parallelize(List (("a" , Array(3)), ("b" , Array(2))))
val res=aul.union(au2)
res.collect


-----

Q23

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"}, 3}
val b = a.keyBy(_.length)
val c = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","woif","bear","bee"), 3)
val d = c.keyBy(_.length)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)),
(6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)),
(6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)),
(3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))


******

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val b=a.keyBy(_.length)
val c = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","woif","bear","bee"), 3)
val d = c.keyBy(_.length)
val res=b.join(d)
res.collect

----
Q24:

You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Create a table in retailedb with following definition.
CREATE table departments_export (department_id int(11), department_name varchar(45),
created_date T1MESTAMP DEFAULT NOWQ);
2. Now import the data from following directory into departments_export table,
/user/cloudera/departments new


sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments_export \
--export-dir /user/cloudera/departments_new
--input-fileds-terminated-by ','

-----
Q25:

You have been given log generating service as below.
Start_logs (It will generate continuous logs)
Tail_logs (You can check , what logs are being generated)
Stop_logs (It will stop the log service)
Path where logs are generated using above service : /opt/gen_logs/logs/access.log
Now write a flume configuration file named flume3.conf , using that configuration file dumps
logs in HDFS file system in a directory called flumeflume3/%Y/%m/%d/%H/%M
Means every minute new directory should be created). Please us the interceptors to
provide timestamp information, if message header does not have header info.
And also note that you have to preserve existing timestamp, if message contains it. Flume
channel should have following property as well. After every 100 message it should be
committed, use non-durable/faster channel and it should be able to hold maximum 1000
events.

******


-----
Q26:

You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish below assignment.
1. Create a table in hive as below.
create table departments_hive(department_id int, department_name string);
2. Now import data from mysql table departments to this hive table. Please make sure that
data should be visible using below hive command, select" from departments_hive


*****
create table departments_hive(department_id int, department_name string);

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--hive-import \
--hive-table departments_hive -m 1


-----
Q27:

You have been given a spark scala application, which is bundled in
jar named hadoopexam.jar.
Your application class name is com.hadoopexam.MyTask
You want that while submitting your application should launch a driver on one of the cluster
node.
Please complete the following command to submit the application.
spark-submit XXX -master yarn \
YYY SSPARK HOME/lib/hadoopexam.jar 10

*******

xxx: --class com.hadoopexam.MyTask
yyy: --deploy-mode cluster


------
Q28:
 You have been given following code snippet.
val grouped = sc.parallelize(Seq(((1,"two"), List((3,4), (5,6)))))
val flattened = grouped.flatMap {A =>
groupValues.map { value => B }
You need to generate following output.
Hence replace A and B
Array((1,two,3,4),(1,two,5,6))

******

val grouped = sc.parallelize(Seq(((1,"two"), List((3,4), (5,6)))))
val flattened = grouped.flatMap {case(key,groupValues) =>groupValues.map { value => (key._1,key._2,value._1,value._2) }}

flattened.collect



------
Q29:
val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)
val d = c.keyBy(_.length) operationl
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, (String, Option[String]}}] = Array((6,(salmon,Some(salmon))),
(6,(salmon,Some(rabbit))),
(6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))),
(6,(salmon,Some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))),
(3,(dog,Some(dog))), (3,(dog,Some(bee))), (3,(rat,Some(dogg)), (3,(rat,Some(cat)j),
(3,(rat.Some(gnu))). (3,(rat,Some(bee))), (8,(elephant,None)))


*******
val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)
val d = c.keyBy(_.length)
b.leftOuterJoin(d).collect


-------

Q30:

Problem Scenario 94 : You have to run your Spark application on yarn with each executor
20GB and number of executors should be 50. Please replace XXX, YYY, ZZZ
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
-class com.hadoopexam.MyTask \
xxx\
-deploy-mode cluster \ # can be client for client mode
YYY\
ZZZ \
/path/to/hadoopexam.jar \
1000

xxx: --master yarn
yyy: --executor-memory 20g
zzz: --num-executors 50

------
Q31:
Problem Scenario 4: You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.categories
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
Import Single table categories (Subset data} to hive managed table , where category_id
between 1 and 22

***

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--hive-table categories \
--hive-import \
--create-hive-table \
--where 'category_id between 1 and 22' \
-m 1

-------------
Q32:
Problem Scenario 39 : You have been given two files
spark16/file1.txt
1,9,5
2,7,4
3,8,3
spark16/file2.txt
1,g,h
2,i,j
3,k,l
Load these two tiles as Spark RDD and join them to produce the below results
(l,((9,5),(g,h)))
(2, ((7,4), (i,j))) (3, ((8,3), (k,l)))
And write code snippet which will sum the second columns of above joined results (5+4+3).

val data1=sc.textFile("file1")
val data2=sc.textFile("file2")

val m1=data1.map(p=>(p.split(",")(0),(p.split(",")(1),p.split(",")(2).toInt)))
val m2=data2.map(p=>(p.split(",")(0),(p.split(",")(1),p.split(",")(2))))
val res=m1.join(m2)
val sum_data=res.map(p=>(p._2._1._2)).reduce(_+_)

------------

Q33:

Problem Scenario 17 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish below assignment.
1. Create a table in hive as below, create table departments_hiveOl(department_id int,
department_name string, avg_salary int);
2. Create another table in mysql using below statement CREATE TABLE IF NOT EXISTS
departments_hive01(id int, department_name varchar(45), avg_salary int);
3. Copy all the data from departments table to departments_hive01 using insert into
departments_hive01 select a.*, null from departments a;
Also insert following records as below
insert into departments_hive01 values(777, "Not known",1000);
insert into departments_hive01 values(8888, null,1000);
insert into departments_hive01 values(666, null,1100);
4. Now import data from mysql table departments_hive01 to this hive table. Please make
sure that data should be visible using below hive command. Also, while importing if null
value found for department_name column replace it with "" (empty string) and for id column
with -999 select * from departments_hive;



****

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments_hive01 \
--null-string "" \
--null-non-string -999 \
--hive-import \
--hive-table departments_hive0l \
-m 1


----
Q34:

Problem Scenario 21 : You have been given log generating service as below.
startjogs (It will generate continuous logs)
tailjogs (You can check , what logs are being generated)
stopjogs (It will stop the log service)
Path where logs are generated using above service : /opt/gen_logs/logs/access.log
Now write a flume configuration file named flumel.conf , using that configuration file dumps
logs in HDFS file system in a directory called flumel. Flume channel should have following
property as well. After every 100 message it should be committed, use non-durable/faster
channel and it should be able to hold maximum 1000 events
Solution :
Step 1 : Create flume configuration file, with below configuration for source, sink and
channel.
#Define source , sink , channel and agent,
agent1 .sources = source1
agent1 .sinks = sink1
agent1.channels = channel1
# Describe/configure source1
agent1 .sources.source1.type = exec
agent1.sources.source1.command = tail -F /opt/gen logs/logs/access.log
## Describe sinkl
agentl .sinks.sinkl.channel = memory-channel
agentl .sinks.sinkl .type = hdfs
agentl .sinks.sink1.hdfs.path = flumel
agentl .sinks.sinkl.hdfs.fileType = Data Stream
# Now we need to define channell property.
agent1.channels.channel1.type = memory
agent1.channels.channell.capacity = 1000
agent1.channels.channell.transactionCapacity = 100
# Bind the source and sink to the channel
agent1.sources.source1.channels = channel1
agent1.sinks.sink1.channel = channel1
Step 2 : Run below command which will use this configuration file and append data in
hdfs.
Start log service using : startjogs
Start flume service:
flume-ng agent -conf /home/cloudera/flumeconf -conf-file
/home/cloudera/flumeconf/flumel.conf-Dflume.root.logger=DEBUG,INFO,console
Wait for few mins and than stop log service.
Stop_logs

******

--------
Q35:

Problem Scenario 67 : You have been given below code snippet.
lines = sc.parallelize(['lts fun to have fun,','but you have to know how.'])
M = lines.map( lambda x: x.replace(',7 ').replace('.',' 'J.replaceC-V ').lower())
r2 = r1.flatMap(lambda x: x.split())
r3 = r2.map(lambda x: (x, 1))
operation1
r5 = r4.map(lambda x:(x[1],x[0]))
r6 = r5.sortByKey(ascending=False)
r6.take(20)
Write a correct code snippet for operationl which will produce desired output, shown below.
[(2, 'fun'), (2, 'to'), (2, 'have'), (1, its'), (1, 'know1), (1, 'how1), (1, 'you'), (1, 'but')]



******
r4=r3.reduceByKey(lambda x,y:x+y)

----------------
Q36:

Problem Scenario 25 : You have been given below comma separated employee
information. That needs to be added in /home/cloudera/flumetest/in.txt file (to do tail
source)
sex,name,city
1,alok,mumbai
1,jatin,chennai
1,yogesh,kolkata
2,ragini,delhi
2,jyotsana,pune
1,valmiki,banglore
Create a flume conf file using fastest non-durable channel, which write data in hive
warehouse directory, in two separate tables called flumemaleemployee1 and
flumefemaleemployee1
(Create hive table as well for given data}. Please use tail source with
/home/cloudera/flumetest/in.txt file.
Flumemaleemployee1 : will contain only male employees data flumefemaleemployee1 :
Will contain only woman employees data

*********


-----
Q37:

Problem Scenario 37 : ABCTECH.com has done survey on their Exam Products feedback
using a web based form. With the following free text field as input in web ui.
Name: String
Subscription Date: String
Rating : String
And servey data has been saved in a file called spark9/feedback.txt
Christopher|Jan 11, 2015|5
Kapil|11 Jan, 2015|5
Thomas|6/17/2014|5
John|22-08-2013|5
Mithun|2013|5
Jitendra||5
Write a spark program using regular expression which will filter all the valid dates and save
in two separate file (good record and bad record)


********

------

Q38:
Problem Scenario GG : You have been given below code snippet.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("ant", "falcon", "squid"), 2)
val d = c.keyBy(.length)
operation 1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String)] = Array((4,lion))


*********


------
Q38:

	Problem Scenario GG : You have been given below code snippet.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("ant", "falcon", "squid"), 2)
val d = c.keyBy(.length)
operation 1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String)] = Array((4,lion))

***

val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("ant", "falcon", "squid"), 2)
val d = c.keyBy(_.length)
b.subtractByKey(d).collect

------
Q39:
Problem Scenario 22 : You have been given below comma separated employee
information.
name,salary,sex,age
alok,100000,male,29
jatin,105000,male,32
yogesh,134000,male,39
ragini,112000,female,35
jyotsana,129000,female,39
valmiki,123000,male,29
Use the netcat service on port 44444, and nc above data line by line. Please do the
following activities.
1. Create a flume conf file using fastest channel, which write data in hive warehouse
directory, in a table called flumeemployee (Create hive table as well tor given data).
2. Write a hive query to read average salary of all employees.



********
Q40:

Problem Scenario 8 : You have been given following mysql database details as well as
other info.
Please accomplish following.
1. Import joined result of orders and order_items table join on orders.order_id =
order_items.order_item_order_id.
2. Also make sure each tables file is partitioned in 2 files e.g. part-00000, part-00002
3. Also make sure you use orderid columns for sqoop to use for boundary conditions.



sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--query 'select * from orders join order_items where orders.order_id =order_items.order_item_order_id' \
--target-dir order_join \
--split-by order_id \
-m 2



-----

Q41:
Problem Scenario 70 : Write down a Spark Application using Python, In which it read a
file "Content.txt" (On hdfs) with following content. Do the word count and save the
results in a directory called "problem85" (On hdfs)
Content.txt
Hello this is ABCTECH.com
This is XYZTECH.com
Apache Spark Training
This is Spark Learning Session
Spark is faster than MapReduce


*****
val data=sc.textFile("path")
val wc=data.flatMap(p=>(p.split(" "))).map(p=>(p,1)).reduceByKey((x,y)=>x+y)
wc.collect
wc.saveAsTextFile("path")



---------
Q42:

Problem Scenario 85 : In Continuation of previous question, please accomplish following
activities.
1. Select all the columns from product table with output header as below. productID AS ID
code AS Code name AS Description price AS 'Unit Price'
2. Select code and name both separated by ' -' and header name should be Product
Description'.
3. Select all distinct prices.
4. Select distinct price and name combination.
5. Select all price data sorted by both code and productID combination.
6. count number of products.
7. Count number of products for each code.


products data queries



-------
Q43:

Problem Scenario 77 : You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.orders
table=retail_db.order_items
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of order table : (orderid , order_date , order_customer_id, order_status)
Columns of ordeMtems table : (order_item_id , order_item_order_ld ,
order_item_product_id, order_item_quantity,order_item_subtotal,order_
item_product_price)
Please accomplish following activities.
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory
p92_orders and p92 order items .
2. Join these data using orderid in Spark and Python
3. Calculate total revenue perday and per order
4. Calculate total and average revenue for each date. - combineByKey
-aggregateByKey


--------
Q44:

Problem Scenario 49 : You have been given below code snippet (do a sum of values by
key}, with intermediate output.
val keysWithValuesList = Array("foo=A", "foo=A", "foo=A", "foo=A", "foo=B", "bar=C",
"bar=D", "bar=D")
val data = sc.parallelize(keysWithValuesl_ist}
//Create key value pairs
val kv = data.map(_.split("=")).map(v => (v(0), v(l))).cache()
val initialCount = 0;
val countByKey = kv.aggregateByKey(initialCount)(addToCounts, sumPartitionCounts)
Now define two functions (addToCounts, sumPartitionCounts) such, which will
produce following results.
Output 1
countByKey.collect
res3: Array[(String, Int)] = Array((foo,5), (bar,3))
import scala.collection._
val initialSet = scala.collection.mutable.HashSet.empty[String]
val uniqueByKey = kv.aggregateByKey(initialSet)(addToSet, mergePartitionSets)
Now define two functions (addToSet, mergePartitionSets) such, which will produce
following results.
Output 2:
uniqueByKey.collect
res4: Array[(String, scala.collection.mutable.HashSet[String])] = Array((foo,Set(B, A}},
(bar,Set(C, D}}}


********



------
Q45:

Problem Scenario 93 : You have to run your Spark application with locally 8 thread or
locally on 8 cores. Replace XXX with correct values.
spark-submit --class com.hadoopexam.MyTask XXX \ -deploy-mode cluster
SSPARK_HOME/lib/hadoopexam.jar 10


xxx: --master local[8]

------

Q46:

Problem Scenario 74 : You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.orders
table=retail_db.order_items
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of order table : (orderjd , order_date , ordercustomerid, order status}
Columns of orderjtems table : (order_item_td , order_item_order_id ,
order_item_product_id,
order_item_quantity,order_item_subtotal,order_item_product_price)
Please accomplish following activities.
1. Copy "retaildb.orders" and "retaildb.orderjtems" table to hdfs in respective directory
p89_orders and p89_order_items .
2. Join these data using orderjd in Spark and Python
3. Now fetch selected columns from joined data Orderld, Order date and amount collected
on this order.
4. Calculate total order placed for each date, and produced the output sorted by date.


*******


-----

Q47:
Problem Scenario 42 : You have been given a file (sparklO/sales.txt), with the content as
given in below.
spark10/sales.txt
Department,Designation,costToCompany,State
Sales,Trainee,12000,UP
Sales,Lead,32000,AP
Sales,Lead,32000,LA
Sales,Lead,32000,TN
Sales,Lead,32000,AP
Sales,Lead,32000,TN
Sales,Lead,32000,LA
Sales,Lead,32000,LA
Marketing,Associate,18000,TN
Marketing,Associate,18000,TN
HR,Manager,58000,TN
And want to produce the output as a csv with group by Department,Designation,State
with additional columns with sum(costToCompany) and TotalEmployeeCountt
Should get result like
Dept,Desg,state,empCount,totalCost
Sales,Lead,AP,2,64000
Sales.Lead.LA.3.96000
Sales,Lead,TN,2,64000




******
val data=sc.textFile("/user/cloudera/practice/Q47/fileq47")
val df=data.map(p=>(p.split(",")(0),p.split(",")(1),p.split(",")(2).toInt,p.split(",")(3))).toDF("dept","desgn","cost","state")
df.registerTempTable("sales")
val res=sqlContext.sql("select dept,desgn,state,count(*) as empcount,sum(cost) as totalcost from sales group by dept,desgn,state having totalcost>60000")
res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)+","+p(4)).coalesce(1).saveAsTextFile("/user/cloudera/practice/Q47/res4")

-----------

Q48:

Problem Scenario 76 : You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.orders
table=retail_db.order_items
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of order table : (orderid , order_date , ordercustomerid, order_status}
.....
Please accomplish following activities.
1. Copy "retail_db.orders" table to hdfs in a directory p91_orders.
2. Once data is copied to hdfs, using pyspark calculate the number of order for each status.
3. Use all the following methods to calculate the number of order for each status. (You
need to know all these functions and its behavior for real exam)
- countByKey()
-groupByKey()
- reduceByKey()
-aggregateByKey()
- combineByKey()


***********

------
Q49:

Problem Scenario 88 : You have been given below three files
product.csv (Create this file in hdfs)
productID,productCode,name,quantity,price,supplierid
1001,PEN,Pen Red,5000,1.23,501
1002,PEN,Pen Blue,8000,1.25,501
1003,PEN,Pen Black,2000,1.25,501
1004,PEC,Pencil 2B,10000,0.48,502
1005,PEC,Pencil 2H,8000,0.49,502
1006,PEC,Pencil HB,0,9999.99,502
2001,PEC,Pencil 3B,500,0.52,501
2002,PEC,Pencil 4B,200,0.62,501
2003,PEC,Pencil 5B,100,0.73,501
2004,PEC,Pencil 6B,500,0.47,502



supplier.csv
supplierid,name,phone
501,ABC Traders,88881111
502,XYZ Company,88882222
503,QQ Corp,88883333
products_suppliers.csv
productID,supplierID
2001,501
2002,501
2003,501
2004,502
2001,503
Now accomplish all the queries given in solution.
1. It is possible that, same product can be supplied by multiple supplier. Now find each
product, its price according to each supplier.
2. Find all the supllier name, who are supplying 'Pencil 3B'
3. Find all the products , which are supplied by ABC Traders.


val products=sc.textFile("/user/cloudera/practice/Q49/product")
val pdf=products.map(p=>(p.split(",")(0).toInt,p.split(",")(1),p.split(",")(2),p.split(",")(3).toInt,p.split(",")(4).toDouble,p.split(",")(5).toInt)).toDF("pid","code","name","quan","price","sid")
pdf.registerTempTable("products")

val supplier=sc.textFile("/user/cloudera/practice/Q49/supplier")
val sdf=supplier.map(p=>(p.split(",")(0).toInt,p.split(",")(1),p.split(",")(2))).toDF("ssid","sname","phone")
sdf.registerTempTable("supplier")

val products_suppliers=sc.textFile("/user/cloudera/practice/Q49/products_suppliers")
val psdf=products_suppliers.map(p=>(p.split(",")(0).toInt,p.split(",")(0).toInt)).toDF("pspid","psid")
psdf.registerTempTable("pds")


val res1=sqlContext.sql("select pid,sname,sum(price) as price from products join supplier on sid==ssid group by pid,sname ")

val res2=sqlContext.sql("select name,sname from products join supplier on sid==ssid and name= 'Pencil 3B'")

val res3=sqlContext.sql("select name from products join supplier on sid==ssid and sname= 'ABC Traders'")





************

-----
Q50:

Problem Scenario 55 : You have been given below code snippet.
val pairRDDI = sc.parallelize(List( ("cat",2), ("cat", 5), ("book", 4),("cat", 12))) 
val pairRDD2 = sc.parallelize(List( ("cat",2), ("cup", 5), ("mouse", 4),("cat", 12)))
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(String, (Option[lnt], Option[lnt]))] = Array((book,(Some(4},None)),
(mouse,(None,Some(4))), (cup,(None,Some(5))), (cat,(Some(2),Some(2)),
(cat,(Some(2),Some(12))), (cat,(Some(5),Some(2))), (cat,(Some(5),Some(12))),
(cat,(Some(12),Some(2))), (cat,(Some(12),Some(12)))J


**********

val pairRDDI = sc.parallelize(List( ("cat",2), ("cat", 5), ("book", 4),("cat", 12))) 
val pairRDD2 = sc.parallelize(List( ("cat",2), ("cup", 5), ("mouse", 4),("cat", 12)))
pairRDDI.fullOuterJoin(pairRDD2).collect


------------
Q51:

Problem Scenario 72 : You have been given a table named "employee2" with following
detail.
first_name string
last_name string
Write a spark script in python which read this table and print all the rows and individual
column values.
***********
val hc=new org.apche.spark.sql.hive.HiveContext(sc)
hc.sql("select * from employee2")


---------
Q52:

Problem Scenario 86 : In Continuation of previous question, please accomplish following
activities.
1. Select Maximum, minimum, average , Standard Deviation, and total quantity.
2. Select minimum and maximum price for each product code.
3. Select Maximum, minimum, average , Standard Deviation, and total quantity for each
product code, hwoever make sure Average and Standard deviation will have maximum two
decimal values.
4. Select all the product code and average price only where product count is more than or
equal to 3.
5. Select maximum, minimum , average and total of all the products for each code. Also
produce the same across all the products.



*******

------
Q53:
Problem Scenario 69 : Write down a Spark Application using Python,
In which it read a file "Content.txt" (On hdfs) with following content.
And filter out the word which is less than 2 characters and ignore all empty lines.
Once doen store the filtered data in a directory called "problem84" (On hdfs)
Content.txt
Hello this is ABCTECH.com
This is ABYTECH.com
Apache Spark Training
This is Spark Learning Session
Spark is faster than MapReduce


******

create  folder hdfs dfs -mkdir /user/cloudera/practice/Q53

place the file in that location.

val data=sc.textFile("/user/cloudera/practice/Q53")
val mdata=data.flatMap(p=>p.split(" "))
val fdata=mdata.filter(p=>(p.length>2))
fdata.saveAsTextFile("path")


---
Q54:

Problem Scenario 87 : You have been given below three files
product.csv (Create this file in hdfs)
productID,productCode,name,quantity,price,supplierid
1001,PEN,Pen Red,5000,1.23,501
1002,PEN,Pen Blue,8000,1.25,501
1003,PEN,Pen Black,2000,1.25,501
1004,PEC,Pencil 2B,10000,0.48,502
1005,PEC,Pencil 2H,8000,0.49,502
1006,PEC,Pencil HB,0,9999.99,502
2001,PEC,Pencil 3B,500,0.52,501
2002,PEC,Pencil 4B,200,0.62,501
2003,PEC,Pencil 5B,100,0.73,501
2004,PEC,Pencil 6B,500,0.47,502
supplier.csv
supplierid,name,phone
501,ABC Traders,88881111
502,XYZ Company,88882222
503,QQ Corp,88883333
products_suppliers.csv
productID,supplierID
2001,501
2002,501
2003,501
2004,502
2001,503
Now accomplish all the queries given in solution.
Select product, its price , its supplier name where product price is less than 0.6 using
SparkSQL


******

continue to the Q49:


val res4=sqlContext.sql("select pid,price,sname from products join supplier on sid==ssid where price<0.6 ")




----------------

Q55:

Problem Scenario 90 : You have been given below two files
course.txt
id,course
1,Hadoop
2,Spark
3,HBase
fee.txt
id,fee
2,3900
3,4200
4,2900
Accomplish the following activities.
1. Select all the courses and their fees , whether fee is listed or not.
2. Select all the available fees and respective course. If course does not exists still list the
fee
3. Select all the courses and their fees , whether fee is listed or not. However, ignore
records having fee as null.

val course=sc.textFile("course")
val cdf=course.map(p=>(p.split(",")(0).toInt,p.split(",")(1))).toDF("cid","cname")
cdf.registerTempTable("course")
val fee=sc.textFile("fee")
val fdf=fee.map(p=>(p.split(",")(0).toInt,p.split(",")(0).toDouble)).toDF("fid","fee")
fdf.registerTempTable("fee")

val res1=sqlContext.sql("select cname,fee from course left join fee on cid=fid")

val res2=sqlContext.sql("select fee,cname from fee left join course on fid=cid")

val res3=sqlContext.sql("select fee,cname from fee  join course on fid=cid")



-----------

Q56:

Problem Scenario 47 : You have been given below code snippet, with intermediate output.
val z = sc.parallelize(List(1,2,3,4,5,6), 2)
// lets first print out the contents of the RDD with partition labels
def myfunc(index: Int, iter: lterator[(lnt)]): lterator[String] = {
iter.toList.map(x => "[partID:" + index + ", val: " + x + "]").iterator
//In each run , output could be different, while solving problem assume belowm output only.
z.mapPartitionsWithlndex(myfunc).collect
res28: Array[String] = Array([partlD:0, val: 1], [partlD:0, val: 2], [partlD:0, val: 3], [partlD:1,
val: 4], [partlD:1, val: S], [partlD:1, val: 6])
Now apply aggregate method on RDD z , with two reduce function , first will select
max value in each partition and second will add all the maximum values from all
partitions.
Initialize the aggregate with value 5. hence expected output will be 16.


***


------
Q57:

Problem Scenario 73 : You have been given data in json format as below.
{"first_name":"Ankit", "last_name":"Jain"}
{"first_name":"Amir", "last_name":"Khan"}
{"first_name":"Rajesh", "last_name":"Khanna"}
{"first_name":"Priynka", "last_name":"Chopra"}
{"first_name":"Kareena", "last_name":"Kapoor"}
{"first_name":"Lokesh", "last_name":"Yadav"}
Do the following activity
1. create employee.json file locally.
2. Load this file on hdfs
3. Register this data as a temp table in Spark using Python.
4. Write select query and print this data.
5. Now save back this selected data in json format.

*************
val data=sqlContext.read.json("/user/cloudera/practice/Q57")
data.registerTempTable("employee")
val res=sqlContext.sql("select * from employee")
res.write.json("/user/cloudera/practice/Q57/res")



----------
Q58:

Problem Scenario 5 : You have been given following mysql database details.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. List all the tables using sqoop command from retail_db
2. Write simple sqoop eval command to check whether you have permission to read
database tables or not.
3. Import all the tables as avro files in /user/hive/warehouse/retail_cca174.db
4. Import departments table as a text file in /user/cloudera/departments.




sqoop list-tables --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \


sqoop eval  --connect jdbc:mysql://quickstart/retail_db --username retail_dba --password cloudera --query "select count(*) from departments"


sqoop import-all-tables --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--warehouse-dir /user/hive/warehouse/retail_cca174.db
--as-avrodatafile \
-m 1


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--as-textfile \
--target-dir /user/cloudera/departments \
-m 1

-------------------
Q59:

Problem Scenario 84 : In Continuation of previous question, please accomplish following
activities.
1. Select all the products which has product code as null
2. Select all the products, whose name starts with Pen and results should be order by Price
descending order.
3. Select all the products, whose name starts with Pen and results should be order by
Price descending order and quantity ascending order.
4. Select top 2 products by price


val products=sc.textFile("/user/cloudera/practice/Q49/product")
val pdf=products.map(p=>(p.split(",")(0).toInt,p.split(",")(1),p.split(",")(2),p.split(",")(3).toInt,p.split(",")(4).toDouble,p.split(",")(5).toInt)).toDF("pid","code","name","quan","price","sid")
pdf.registerTempTable("products")

val supplier=sc.textFile("/user/cloudera/practice/Q49/supplier")
val sdf=supplier.map(p=>(p.split(",")(0).toInt,p.split(",")(1),p.split(",")(2))).toDF("ssid","sname","phone")
sdf.registerTempTable("supplier")

val products_suppliers=sc.textFile("/user/cloudera/practice/Q49/products_suppliers")
val psdf=products_suppliers.map(p=>(p.split(",")(0).toInt,p.split(",")(0).toInt)).toDF("pspid","psid")
psdf.registerTempTable("pds")



val res1=sqlContext.sql("select * from products where code is null")

val res2=sqlContext.sql("select * from products where name like 'Pen%' order by price desc)

val res3=sqlContext.sql("select * from products where name like 'Pen%' order by price desc,quan asc")

val res4=sqlContext.sql("select  * from products order by price desc limit 2")



-----

Q60:

Problem Scenario 30 : You have been given three csv files in hdfs as below.
EmployeeName.csv with the field (id, name)
EmployeeManager.csv (id, manager Name)
EmployeeSalary.csv (id, Salary)
Using Spark and its API you have to generate a joined output as below and save as a text
tile (Separated by comma) for final distribution and output must be sorted by id.
ld,name,salary,managerName
EmployeeManager.csv
E01,Vishnu
E02,Satyam
E03,Shiv
E04,Sundar
E05,John
E06,Pallavi
E07,Tanvir
E08,Shekhar
E09,Vinod
E10,Jitendra
EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat
EmployeeSalary.csv
E01,50000
E02,50000
E03,45000
E04,45000
E05,50000
E06,45000
E07,50000
E08,10000
E09,10000
E10,10000

***
val em=sc.textFile("/user/cloudera/practice/Q60/EmployeeManager.csv")

val emf=em.map(p=>(p.split(",")(0),p.split(",")(1))).toDF("emid","em")
emf.registerTempTable("emf")
val en=sc.textFile("/user/cloudera/practice/Q60/EmployeeName.csv")
val enf=en.map(p=>(p.split(",")(0),p.split(",")(1))).toDF("enid","en")
enf.registerTempTable("enf")
val es=sc.textFile("/user/cloudera/practice/Q60/EmployeeSalary.csv")
val esf=es.map(p=>(p.split(",")(0),p.split(",")(1).toFloat)).toDF("esid","es")
esf.registerTempTable("esf")

val res=sqlContext.sql("select emid as id,en as name,es as salary,em as managerName from emf join enf on emid=enid join esf on emid=esid  order by  1 desc")



-----
Q61:

Problem Scenario 24 : You have been given below comma separated employee
information.
Data Set:
name,salary,sex,age
alok,100000,male,29
jatin,105000,male,32
yogesh,134000,male,39
ragini,112000,female,35
jyotsana,129000,female,39
valmiki,123000,male,29
Requirements:
Use the netcat service on port 44444, and nc above data line by line. Please do the
following activities.
1. Create a flume conf file using fastest channel, which write data in hive warehouse
directory, in a table called flumemaleemployee (Create hive table as well tor given data).
2. While importing, make sure only male employee data is stored.


-------

Q62:

Problem Scenario 27 : You need to implement near real time solutions for collecting
information when submitted in file with below information.
Data
echo "IBM,100,20160104" >> /tmp/spooldir/bb/.bb.txt
echo "IBM,103,20160105" >> /tmp/spooldir/bb/.bb.txt
mv /tmp/spooldir/bb/.bb.txt /tmp/spooldir/bb/bb.txt
After few mins
echo "IBM,100.2,20160104" >> /tmp/spooldir/dr/.dr.txt
echo "IBM,103.1,20160105" >> /tmp/spooldir/dr/.dr.txt
mv /tmp/spooldir/dr/.dr.txt /tmp/spooldir/dr/dr.txt
Requirements:
You have been given below directory location (if not available than create it) /tmp/spooldir .
You have a finacial subscription for getting stock prices from BloomBerg as well as
Reuters and using ftp you download every hour new files from their respective ftp site in
directories /tmp/spooldir/bb and /tmp/spooldir/dr respectively.
As soon as file committed in this directory that needs to be available in hdfs in
/tmp/flume/finance location in a single directory.
Write a flume configuration file named flume7.conf and use it to load data in hdfs with
following additional properties .
1. Spool /tmp/spooldir/bb and /tmp/spooldir/dr
2. File prefix in hdfs sholuld be events
3. File suffix should be .log
4. If file is not commited and in use than it should have _ as prefix.
5. Data should be written as text to hdfs

------

Q63:

Problem Scenario 57 : You have been given below code snippet.
val a = sc.parallelize(1 to 9, 3) operationl
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(String, Seq[lnt])] = Array((even,ArrayBuffer(2, 4, G, 8)), (odd,ArrayBuffer(1, 3, 5, 7,
9)))

*****
val a = sc.parallelize(1 to 9, 3)
a.groupBy(p=>{if(p%2==0)"even" else "odd"})


----
Q64:
Problem Scenario 34 : You have given a file named spark6/user.csv.
Data is given below:
user.csv
id,topic,hits
Rahul,scala,120
Nikita,spark,80
Mithun,spark,1
myself,cca175,180
Now write a Spark code in scala which will remove the header part and create RDD of
values as below, for all rows. And also if id is myself" than filter out row.
Map(id -> om, topic -> scala, hits -> 120)


********

-----
Q65:

Problem Scenario 91 : You have been given data in json format as below.
{"first_name":"Ankit", "last_name":"Jain"}
{"first_name":"Amir", "last_name":"Khan"}
{"first_name":"Rajesh", "last_name":"Khanna"}
{"first_name":"Priynka", "last_name":"Chopra"}
{"first_name":"Kareena", "last_name":"Kapoor"}
{"first_name":"Lokesh", "last_name":"Yadav"}
Do the following activity
1. create employee.json tile locally.
2. Load this tile on hdfs
3. Register this data as a temp table in Spark using Python.
4. Write select query and print this data.
5. Now save back this selected data in json format.

repeated quetion (Q57)


-----------
Q66:

Problem Scenario 2 :
There is a parent organization called "ABC Group Inc", which has two child companies
named Tech Inc and MPTech.
Both companies employee information is given in two separate text file as below. Please do
the following activity for employee details.
Tech Inc.txt
1,Alok,Hyderabad
2,Krish,Hongkong
3,Jyoti,Mumbai
4,Atul,Banglore
5,Ishan,Gurgaon
MPTech.txt
6,John,Newyork
7,alp2004,California
8,tellme,Mumbai
9,Gagan21,Pune
10,Mukesh,Chennai
1. Which command will you use to check all the available command line options on HDFS
and How will you get the Help for individual command.
2. Create a new Empty Directory named Employee using Command line. And also create
an empty file named in it Techinc.txt
3. Load both companies Employee data in Employee directory (How to override existing file
in HDFS).
4. Merge both the Employees data in a Single tile called MergedEmployee.txt, merged tiles
should have new line character at the end of each file content.
5. Upload merged file on HDFS and change the file permission on HDFS merged file, so
that owner and group member can read and write, other user can read the file.
6. Write a command to export the individual file as well as entire directory from HDFS to
local file System.





********

1: hdfs dfs ,hdfs dfs -help <command name>
2: hdfs dfs -mkdir /user/cloudera/practice/Q66 and create empty file using hue file browser
3: create files in the local and then copy to hdfs to above created files



hdfs dfs -put -f /home/cloudera/Employee/*.csv /user/cloudera/practice/Q66

4: hdfs dfs -getmerge -nl /user/cloudera/practice/Q66 MergedEmployee.txt

merged file will be in local

copy it to hdfs 

hdfs dfs -put MergedEmployee.txt /user/cloudera/practice/Q66

hdfs dfs -chmod 664 /user/cloudera/practice/Q66/MergedEmployee.txt

hdfs dfs -get /user/cloudera/practice/Q66 Employee_hdfs



------
Q67:

Problem Scenario 9 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Import departments table in a directory.
2. Again import departments table same directory (However, directory already exist hence
it should not overrride and append the results)
3. Also make sure your results fields are terminated by '|' and lines terminated by '\n\


****


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/practice/Q67 \
-m 1


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/practice/Q67 \
--append \
--fields-terminated-by '|' \
-m 1


----------

Q68

Problem Scenario 68 : You have given a file as below.
spark75/f ile1.txt
File contain some text. As given Below
spark75/file1.txt
Apache Hadoop is an open-source software framework written in Java for distributed
storage and distributed processing of very large data sets on computer clusters built from
commodity hardware. All the modules in Hadoop are designed with a fundamental
assumption that hardware failures are common and should be automatically handled by the
framework
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File
System (HDFS) and a processing part called MapReduce. Hadoop splits files into large
blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers
packaged code for nodes to process in parallel based on the data that needs to be
processed.
his approach takes advantage of data locality nodes manipulating the data they have
access to to allow the dataset to be processed faster and more efficiently than it would be
in a more conventional supercomputer architecture that relies on a parallel file system
where computation and data are distributed via high-speed networking
For a slightly more complicated task, lets look into splitting up sentences from our
documents into word bigrams. A bigram is pair of successive tokens in some sequence.
We will look at building bigrams from the sequences of words in each sentence, and then
try to find the most frequently occuring ones.
The first problem is that values in each partition of our initial RDD describe lines from the
file rather than sentences. Sentences may be split over multiple lines. The glom() RDD
method is used to create a single entry for each document containing the list of all lines, we
can then join the lines up, then resplit them into sentences using "." as the separator, using
flatMap so that every object in our RDD is now a sentence.
A bigram is pair of successive tokens in some sequence. Please build bigrams from the
sequences of


****


-----

Q69:

Problem Scenario 64 : You have been given below code snippet.
val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(Ust("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)
val d = c.keyBy(_.length)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, (Option[String], String))] = Array((6,(Some(salmon),salmon)),
(6,(Some(salmon),rabbit}}, (6,(Some(salmon),turkey)), (6,(Some(salmon),salmon)),
(6,(Some(salmon),rabbit)), (6,(Some(salmon),turkey)), (3,(Some(dog),dog)),
(3,(Some(dog),cat)), (3,(Some(dog),gnu)), (3,(Some(dog),bee)), (3,(Some(rat),
(3,(Some(rat),cat)), (3,(Some(rat),gnu)), (3,(Some(rat),bee)), (4,(None,wo!f)),
(4,(None,bear)))

****

val res=b.rightOuterJoin(d)



---------

Q70:

Problem Scenario 56 : You have been given below code snippet.
val a = sc.parallelize(l to 100, 3)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array [Array [I nt]] = Array(Array(1, 2, 3,4, 5, 6, 7, 8, 9,10,11,12,13,14,15,16,17,18,19, 20,
21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33),
Array(34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,
56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66),
Array(67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88,
89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100))


****

a.glom.collect



------
Q71:

Problem Scenario 31 : You have given following two files
1. Content.txt: Contain a huge text file containing space separated words.
2. Remove.txt: Ignore/filter all the words given in this file (Comma Separated).
Write a Spark program which reads the Content.txt file and load as an RDD, remove all the
words from a broadcast variables (which is loaded as an RDD of words from Remove.txt).
And count the occurrence of the each word and save it as a text file in HDFS.
Content.txt
Hello this is ABCTech.com
This is TechABY.com
Apache Spark Training
This is Spark Learning Session
Spark is faster than MapReduce
Remove.txt
Hello, is, this, the

****

val data1=sc.textFile("path1")
val data2=sc.textFile("path2")

val d1=data1.flatMap(p=>p.split(" ")).map(p=>p.trim)
val d2=data2.map(p=>p.split(",")).map(p=>p.trim)
val res=d1.subtract(d2).reduceByKey(_+_)
res.saveAsTextFile("path")

----------
Q72:

Problem Scenario 3: You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.categories
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. Import data from categories table, where category=22 (Data should be stored in
categories subset)
2. Import data from categories table, where category>22 (Data should be stored in
categories_subset_2)
3. Import data from categories table, where category between 1 and 22 (Data should be
stored in categories_subset_3)
4. While importing catagories data change the delimiter to '|' (Data should be stored in
categories_subset_S)
5. Importing data from catagories table and restrict the import to category_name,category
id columns only with delimiter as '|'
6. Add null values in the table using below SQL statement ALTER TABLE categories
modify category_department_id int(11); INSERT INTO categories values
(eO.NULL.'TESTING');
7. Importing data from catagories table (In categories_subset_17 directory) using '|'
delimiter and categoryjd between 1 and 61 and encode null values for both string and non
string columns.
8. Import entire schema retail_db in a directory categories_subset_all_tables


****

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--where 'categoryid=22' \
--target-dir /user/cloudera/practice/Q72/categories_subset_1 \
-m 1


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--where 'categoryid>22' \
--target-dir /user/cloudera/practice/Q72/categories_subset_2 \
-m 1


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--where 'categoryid between 1 and 22' \
--target-dir /user/cloudera/practice/Q72/categories_subset_3 \
-m 1



sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--target-dir /user/cloudera/practice/Q72/categories_subset_5 \
--fields-terminated-by '|' \
-m 1


7.

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table categories \
--target-dir /user/cloudera/practice/Q72/categories_subset_7 \
--where 'categoryid between 1 and 61' \
--fields-terminated-by '|' \
--null-string '' \
--null-non-string -6 \
-m 1


sqoop import-all-tables --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--warehouse-dir /user/cloudera/practice/Q72/categories_subset_8


------
Q73:

Problem Scenario 71 :
Write down a Spark script using Python,
In which it read a file "Content.txt" (On hdfs) with following content.
After that split each row as (key, value), where key is first word in line and entire line as
value.
Filter out the empty lines.
And save this key value in "problem86" as Sequence file(On hdfs)
Part 2 : Save as sequence file , where key as null and entire line as value. Read back the
stored sequence files.
Content.txt
Hello this is ABCTECH.com
This is XYZTECH.com
Apache Spark Training
This is Spark Learning Session
Spark is faster than MapReduce



*****

contentRDD = sc.textFile("/user/ssettipalli/practice/Content.txt")
nonemptylines = contentRDD.filter(lambda x: len(x) > 0)
nonemptylines.map(lambda line:(None, line)).saveAsSequenceFile("/user/ssettipalli/practice/Q73")
nonemptylines.map(lambda line:(line.split(" ")[0], line)).saveAsSequenceFile("/user/ssettipalli/practice/Q731")

seqRDD = sc.sequenceFile("/user/ssettipalli/practice/Q731")
seqRDD.collect()


------



Q74:

Problem Scenario 45 : You have been given 2 files , with the content as given Below
(spark12/technology.txt)
(spark12/salary.txt)
(spark12/technology.txt)
first,last,technology
Amit,Jain,java
Lokesh,kumar,unix
Mithun,kale,spark
Rajni,vekat,hadoop
Rahul,Yadav,scala
(spark12/salary.txt)
first,last,salary
Amit,Jain,100000
Lokesh,kumar,95000
Mithun,kale,150000
Rajni,vekat,154000
Rahul,Yadav,120000
Write a Spark program, which will join the data based on first and last name and save the
joined results in following format, first Last.technology.salary


*****


val te=sc.textFile("/user/ssettipalli/practice/Q74/technology.txt")
val sal=sc.textFile("/user/ssettipalli/practice/Q74/salary.txt")

val tmap=te.map(p=>((p.split(",")(0),p.split(",")(1)),p.split(",")(2)))
val smap=sal.map(p=>((p.split(",")(0),p.split(",")(1)),p.split(",")(2)))
val jrdd=tmap.join(smap)

val m=jrdd.map(p=>p._1._1+","+p._1._2+","+p._2._1+","+p._2._2)
m.saveAsTextFile("/user/ssettipalli/practice/Q74/res")


----
Q75:

Problem Scenario 54 : You have been given below code snippet.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"))
val b = a.map(x => (x.length, x))
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String)] = Array((4,lion), (7,panther), (3,dogcat), (5,tigereagle))

val res=b.reduceByKey(_+_) 
res.collect


-----

  
-----

Q76:

Problem Scenario 38 : You have been given an RDD as below,
val rdd=RDD[Array[Byte]]
Now you have to save this RDD as a SequenceFile. And below is the code snippet.
import org.apache.hadoop.io.compress.GzipCodec;
val s=rdd.map(bytesArray => (A.get(), newB(bytesArray))).saveAsSequenceFile("7output/path",classOf[GzipCodec])
What would be the correct replacement for A and B in above snippet.
*****
A. NullWritable B. BytesWritable


-----
Q77:
roblem Scenario 51 : You have been given below code snippet.
val a = sc.parallelize(List(1, 2,1, 3), 1)
val b = a.map((_, "b"))
val c = a.map((_, "c"))
Operation_xyz
Write a correct code snippet for Operationxyz which will produce below output.
res:"Array[(lnt, (lterable[String], lterable[String]))] = Array((2,(ArrayBuffer(b),ArrayBuffer(c))),(3,(ArrayBuffer(b),ArrayBuffer(c))),(1,(ArrayBuffer(b, b),ArrayBuffer(c, c)))"


****
val a = sc.parallelize(List(1, 2,1, 3), 1)
val b = a.map((_, "b"))
val c = a.map((_, "c"))
b.cogroup(c).collect

------
Q78:
Problem Scenario 35 : You have been given a file named spark7/EmployeeName.csv
(id,name).
EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat
1. Load this file from hdfs and sort it by name and save it back as (id,name) in results
directory. However, make sure while saving it should be able to write In a single file.


****

val data=sc.textFile("/user/ssettipalli/practice/Q78/")
val md=data.map(p=>(p.split(",")(0),p.split(",")(1)))
val res=md.map(p=>p.swap).sortByKey().map(p=>p.swap)
res.coalesce(1).map(p=>p._1+","+p._2).saveAsTextFile("/user/ssettipalli/practice/Q78/res")

-----
Q79:

Problem Scenario 18 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Now accomplish following activities.
1. Create mysql table as below.
mysql --user=retail_dba -password=cloudera
use retail_db
CREATE TABLE IF NOT EXISTS departments_hive02(id int, department_name
varchar(45), avg_salary int);
show tables;
2. Now export data from hive table departments_hive01 in departments_hive02. While
exporting, please note following. wherever there is a empty string it should be loaded as a
null value in mysql.
wherever there is -999 value for int field, it should be created as null value.



*****

sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments_hive02 \
--export-dir /user/hive/warehouse/departments_hive0l \
--input-fields-terminated-by '\001' \
--input-null-string "" \
--input-null-non-string -999 


-------

Q80:
Problem Scenario 6 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Compression Codec : org.apache.hadoop.io.compress.SnappyCodec
Please accomplish following.
1. Import entire database such that it can be used as a hive tables, it must be created in
default schema.
2. Also make sure each tables file is partitioned in 3 files e.g. part-00000, part-00002, part-
3. Store all the Java files in a directory called java_output to evalute the further


****

sqoop import-all-tables --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--hive-import \
--warehouse-dir /user/hive/warehouse \
-m 3 \
-z \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec \
--outdir java_output

-----

Q81:
Problem Scenario 7 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Import department tables using your custom boundary query, which import departments
between 1 to 25.
2. Also make sure each tables file is partitioned in 2 files e.g. part-00000, part-00002
3. Also make sure you have imported only two columns from table, which are
department_id,department_name

*******

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--where 'department_id between 1 and 25' \
--target-dir /user/cloudera/practice/Q81 \
--columns department_id,department_name \
-m 2

-----
Q82:

Problem Scenario 40 : You have been given sample data as below in a file called
spark15/file1.txt
3070811,1963,1096,,"US","CA",,1,
3022811,1963,1096,,"US","CA",,1,56
3033811,1963,1096,,"US","CA",,1,23
Below is the code snippet to process this tile.
val field= sc.textFile("spark15/f ilel.txt")
val mapper = field.map(x=> A)
mapper.map(x => x.map(x=> {B})).collect
Please fill in A and B so it can generate below final output
Array(Array(3070811,1963,109G, 0, "US", "CA", 0,1, 0)
,Array(3022811,1963,1096, 0, "US", "CA", 0,1, 56)
,Array(3033811,1963,1096, 0, "US", "CA", 0,1, 23)



********


val field=sc.textFile("/user/ssettipalli/practice/Q82")
val mapper = field.map(p=>p.split(","))
mapper.map(x=>x.map(x=> if (x.isEmpty) 0 else x)).collect


--------
Q83:

Problem Scenario 96 : Your spark application required extra Java options as below. -
XX:+PrintGCDetails-XX:+PrintGCTimeStamps
Please replace the XXX values correctly
./bin/spark-submit --name "My app" --master local[4] --conf spark.eventLog.enabled=talse -
-conf XXX hadoopexam.jar



****

Mspark.executoi\extraJavaOptions=-XX:+PrintGCDetails -XX:+PrintGCTimeStamps

----

Q84:

Problem Scenario 33 : You have given a files as below.
spark5/EmployeeName.csv (id,name)
spark5/EmployeeSalary.csv (id,salary)
Data is given below:
EmployeeName.csv
E01,Lokesh
E02,Bhupesh
E03,Amit
E04,Ratan
E05,Dinesh
E06,Pavan
E07,Tejas
E08,Sheela
E09,Kumar
E10,Venkat
EmployeeSalary.csv
E01,50000
E02,50000
E03,45000
E04,45000
E05,50000
E06,45000
E07,50000
E08,10000
E09,10000
E10,10000
Now write a Spark code in scala which will load these two tiles from hdfs and join the same,
and produce the (name.salary) values.
And save the data in multiple tile group by salary (Means each file will have name of
employees with same salary). Make sure file name include salary as well.


*****


val en=sc.textFile("/user/cloudera/practice/Q84/EmployeeName.csv")
val enf=en.map(p=>(p.split(",")(0),p.split(",")(1)))

val es=sc.textFile("/user/cloudera/practice/Q84/EmployeeSalary.csv")
val esf=es.map(p=>(p.split(",")(0),p.split(",")(1)))

val jrdd=enf.join(esf)

val grpByKey =jrdd.map(p=>(p._2._1,p._2._2)).map(_.swap).groupByKey().collect

val rddByKey = grpByKey.map{case (k,v) => k->sc.makeRDD(v.toSeq)}

rddByKey.foreach{ case (k,rdd) => rdd.saveAsTextFile("/user/cloudera/practice/Q84/res/Employee"+k)}

--- testing:
rddByKey.foreach{ case (k,rdd) => rdd.collect.foreach(print)}


------
Q85:

Problem Scenario 14 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. Create a csv file named updated_departments.csv with the following contents in local file
system.
updated_departments.csv
2,fitness
3,footwear
12,fathematics
13,fcience
14,engineering
1000,management
2. Upload this csv file to hdfs filesystem,
3. Now export this data from hdfs to mysql retaildb.departments table. During upload make
sure existing department will just updated and new departments needs to be inserted.
4. Now update updated_departments.csv file with below content.
2,Fitness
3,Footwear
12,Fathematics
13,Science
14,Engineering
1000,Management
2000,Quality Check
5. Now upload this file to hdfs.
6. Now export this data from hdfs to mysql retail_db.departments table. During upload
make sure existing department will just updated and no new departments needs to be
inserted.

*****


sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--export-dir /user/cloudera/practice/Q85 \
--update-key department_id \
--update-mode allowinsert 

edit the file and then run below command once again


sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--export-dir /user/cloudera/practice/Q85 \
--update-key department_id \
--update-mode updateonly


check the mysql table data after this for verification.
2000 recrd shoud not insert

--------

Q86:

Problem Scenario 44 : You have been given 4 files , with the content as given below:
spark11/file1.txt
Apache Hadoop is an open-source software framework written in Java for distributed
storage and distributed processing of very large data sets on computer clusters built from
commodity hardware. All the modules in Hadoop are designed with a fundamental
assumption that hardware failures are common and should be automatically handled by the
framework
spark11/file2.txt
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File
System (HDFS) and a processing part called MapReduce. Hadoop splits files into large
blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers
packaged code for nodes to process in parallel based on the data that needs to be
processed.
spark11/file3.txt
his approach takes advantage of data locality nodes manipulating the data they have
access to to allow the dataset to be processed faster and more efficiently than it would be
in a more conventional supercomputer architecture that relies on a parallel file system
where computation and data are distributed via high-speed networking
spark11/file4.txt
Apache Storm is focused on stream processing or what some call complex event
processing. Storm implements a fault tolerant method for performing a computation or
pipelining multiple computations on an event as it flows into a system. One might use
Storm to transform unstructured data as it flows into a system into a desired format
(spark11Afile1.txt)
(spark11/file2.txt)
(spark11/file3.txt)
(sparkl 1/file4.txt)
Write a Spark program, which will give you the highest occurring words in each file. With
their file name and highest occurring words.


*****
val d1=sc.textFile("/user/cloudera/practice/Q86/file1.txt")
val d2=sc.textFile("/user/cloudera/practice/Q86/file2.txt")
val d3=sc.textFile("/user/cloudera/practice/Q86/file3.txt")
val d4=sc.textFile("/user/cloudera/practice/Q86/file4.txt")


val content1=d1.flatMap(p=>p.split(" ")).map(p=>(p,1)).reduceByKey(_+_).map(_.swap).sortByKey(false).map(_.swap)
val content2=d2.flatMap(p=>p.split(" ")).map(p=>(p,1)).reduceByKey(_+_).map(_.swap).sortByKey(false).map(_.swap)
val content3=d3.flatMap(p=>p.split(" ")).map(p=>(p,1)).reduceByKey(_+_).map(_.swap).sortByKey(false).map(_.swap)
val content4=d4.flatMap(p=>p.split(" ")).map(p=>(p,1)).reduceByKey(_+_).map(_.swap).sortByKey(false).map(_.swap)

val filelword = sc.makeRDD(Array(d1.name+"->"+content1(0)._1+"-"+content1(0)._2)) 
val file2word = sc.makeRDD(Array(d2.name+"->"+content2(0)._1+"-"+content2(0)._2)) 
val file3word = sc.makeRDD(Array(d3.name+"->"+content3(0)._1+"-"+content3(0)._2)) 
val file4word = sc.makeRDD(Array(d4.name+M->"+content4(0)._1+"-"+content4(0)._2))

----
Q87:

Problem Scenario 20 : You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.categories
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. Write a Sqoop Job which will import "retaildb.categories" table to hdfs, in a directory
name "categories_targetJob".


***
sqoop job --create sqoop_job -- import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table catagories \
--target-dir categories_targetJob 

----
Q88:

Problem Scenario 11 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Import departments table in a directory called departments.
2. Once import is done, please insert following 5 records in departments mysql table.
Insert into departments(10, physics);
Insert into departments(11, Chemistry);
Insert into departments(12, Maths);
Insert into departments(13, Science);
Insert into departments(14, Engineering);
3. Now import only new inserted records and append to existring directory . which has been
created in first step.


******
sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--target-dir /user/cloudera/departments \
-m 1



Insert into departments(10, physics);
Insert into departments(11, Chemistry);
Insert into departments(12, Maths);
Insert into departments(13, Science);
Insert into departments(14, Engineering);


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table departments \
--increamental append \
--target-dir /user/cloudera/departments \
--check-column department_id \
--last-value 7


------

Q89:
Problem Scenario 29 : Please accomplish the following exercises using HDFS command
line options.
1. Create a directory in hdfs named hdfs_commands.
2. Create a file in hdfs named data.txt in hdfs_commands.
3. Now copy this data.txt file on local filesystem, however while copying file please make
sure file properties are not changed e.g. file permissions.
4. Now create a file in local directory named data_local.txt and move this file to hdfs in
hdfs_commands directory.
5. Create a file data_hdfs.txt in hdfs_commands directory and copy it to local file system.
6. Create a file in local filesystem named file1.txt and put it to hdfs


********

hdfs dfs -mkdir hdfs_commands 

hdfs dfs -touchz hdfs_commands/data.txt

hdfs dfs -get hdfs_commands/data.txt /home/cloudera/Q89


touch data_local.txt

hdfs dfs -put data_local.txt hdfs_commands/

hdfs dfs -touchz hdfs_commands/data_hdfs.txt

hdfs dfs -get hdfs_commands/data_hdfs.txt /home/cloudera/Q89

touch file1.txt

hdfs dfs -put file1.txt  hdfs_commands/




-------

Q90:

Problem Scenario 75 : You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.orders
table=retail_db.order_items
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. Copy "retail_db.order_items" table to hdfs in respective directory p90_order_items .
2. Do the summation of entire revenue in this table using pyspark.
3. Find the maximum and minimum revenue as well.
4. Calculate average revenue
Columns of ordeMtems table : (order_item_id , order_item_order_id ,
order_item_product_id, order_item_quantity,order_item_subtotal,order_
item_subtotal,order_item_product_price)

******


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table order_items \
--tagert-dir /user/cloudera/practice/Q90 \
-m 1


entireTableRDD = sc.textFile("/user/cloudera/practice/Q90")
extractedRevenueColumn = entireTableRDD.map(lambda line: float(line.split(",")[4]))
totalRevenue = extractedRevenueColumn.reduce(lambda a, b: a + b)
maximumRevenue = extractedRevenueColumn.reduce(lambda a, b: (a if a>=b else b))
minimumRevenue = extractedRevenueColumn.reduce(lambda a, b: (a if a<=b else b))
count=extractedRevenueColumn.count()
averageRev=totalRevenue/count


----
Q91:

Problem Scenario 82 : You have been given table in Hive with following structure (Which
you have created in previous exercise).
productid int code string name string quantity int price float
Using SparkSQL accomplish following activities.
1. Select all the products name and quantity having quantity <= 2000
2. Select name and price of the product having code as 'PEN'
3. Select all the products, which name starts with PENCIL
4. Select all products which "name" begins with P\ followed by any two characters,
followed by space, followed by zero or more characters


****

val products=sc.textFile("/user/cloudera/practice/Q91/product")
val pdf=products.map(p=>(p.split(",")(0).toInt,p.split(",")(1),p.split(",")(2),p.split(",")(3).toInt,p.split(",")(4).toDouble,p.split(",")(5).toInt)).toDF("pid","code","name","quan","price","sid")
pdf.registerTempTable("products")


val res1=sqlContext.sql("select name from products where quan<= 2000")
val res2=sqlContext.sql("select name,price from products where code= 'PEN'")
val res3=sqlContext.sql("select name,price from products where name like 'PENCIL%'")
val res4=sqlContext.sql("select name,price from products where name like 'P_%'")


----
Q92:

Problem Scenario 26 : You need to implement near real time solutions for collecting
information when submitted in file with below information. You have been given below
directory location (if not available than create it) /tmp/nrtcontent. Assume your departments
upstream service is continuously committing data in this directory as a new file (not stream
of data, because it is near real time solution). As soon as file committed in this directory
that needs to be available in hdfs in /tmp/flume location
Data
echo "I am preparing for CCA175 from ABCTECH.com" > /tmp/nrtcontent/.he1.txt
mv /tmp/nrtcontent/.he1.txt /tmp/nrtcontent/he1.txt
After few mins
echo "I am preparing for CCA175 from TopTech.com" > /tmp/nrtcontent/.qt1.txt
mv /tmp/nrtcontent/.qt1.txt /tmp/nrtcontent/qt1.txt
Write a flume configuration file named flumes.conf and use it to load data in hdfs with
following additional properties.
1. Spool /tmp/nrtcontent
2. File prefix in hdfs sholuld be events
3. File suffix should be Jog
4. If file is not commited and in use than it should have as prefix.
5. Data should be written as text to hdfs



****
flume



------

Q93:

Problem Scenario 36 : You have been given a file named spark8/data.csv (type,name).
data.csv
1,Lokesh
2,Bhupesh
2,Amit
2,Ratan
2,Dinesh
1,Pavan
1,Tejas
2,Sheela
1,Kumar
1,Venkat
1. Load this file from hdfs and save it back as (id, (all names of same type)) in results
directory. However, make sure while saving it should be

*******

val name=sc.textFile("/user/ssettipalli/practice/Q93")
val namePairRDD = name.map(x=> (x.split(",")(0),x.split(",")(1))) 
val swapped = namePairRDD.map(item => item.swap) 
val combinedOutput = namePairRDD.combineByKey(List(_), (x:List[String], y:String) => y :: x, (x:List[String], y:List[String]) => x ::: y) 
combinedOutput.repartition(1).saveAsTextFile("/user/ssettipalli/practice/Q93/result.txt")


or 
val name=sc.textFile("/user/ssettipalli/practice/Q93")
val namePairRDD = name.map(x=> (x.split(",")(0),x.split(",")(1))) 	
val gp=namePairRDD.groupByKey
val re=gp.map(p=>(p._1,p._2.toList.sorted))
re.repartition(1).saveAsTextFile("/user/ssettipalli/practice/Q93/result1.txt")

---
Q94:
Problem Scenario 78 : You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.orders
table=retail_db.order_items
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of order table : (orderid , order_date , order_customer_id, order_status)
Columns of ordeMtems table : (order_item_td , order_item_order_id ,
order_item_product_id,
order_item_quantity,order_item_subtotal,order_item_product_price)
Please accomplish following activities.
1. Copy "retail_db.orders" and "retail_db.order_items" table to hdfs in respective directory
p92_orders and p92_order_items .
2. Join these data using order_id in Spark and Python
3. Calculate total revenue perday and per customer
4. Calculate maximum revenue customer

*********

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table orders \
--target-dir /user/cloudera/practice/Q94_orders \
-m 1 


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username retail_dba \
--password cloudera \
--table order_items \
--target-dir /user/cloudera/practice/Q94_order_items \
-m 1 



orders=sc.textFile("/user/cloudera/practice/Q94_orders")

order_items=sc.textFile("/user/cloudera/practice/Q94_order_items")

orderdf=orders.map(lamda x:(x.split(",")[0],x.split(",")[1],x.split(",")[2],x.split(",")[3])).toDF("od","odate","ocid","os")
orderdf.registerTempTable("orders")
orderitemsdf=order_items.map(lamda x:(x.split(",")[0],x.split(",")[1],x.split(",")[2],x.split(",")[3],x.split(",")[4],x.split(",")[5])).toDF("oid","oitod","pid","quan","subtotal","price")
orderitemsdf.registerTempTable("order_items")



-----
Q95:

Problem Scenario 59 : You have been given below code snippet.
val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30) operationl
z.collect
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[lnt] = Array(16,12, 20,13,17,14,18,10,19,15,11)


***

val x = sc.parallelize(1 to 20)
val y = sc.parallelize(10 to 30)
val z = x.intersection(y)


----
Q96:

Problem Scenario 62 : You have been given below code snippet.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val b = a.map(x => (x.length, x))
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String)] = Array((3,xdogx), (5,xtigerx), (4,xlionx), (3,xcatx), (7,xpantherx),
(5,xeaglex))


***

b.mapValues("x"+_+"x").collect







------------------------------cca problems----------------

 

Sqoop import--connect jdbc:mysql://gateway/problem1 \
--username cloudera \
--password cloudera \
--table account \
--target-dir /user/cert/problem1/solution/ \
--where “status <> ‘Archived’”


2.

 

Sqoop export –connect jdbc:mysql://gateway/problem2 \
--username cloudera \
--password cloudera \
--table solution \
--export-dir /user/cert/problem2/customer 

3.

 

Val data=sqlContext.sql(“select * from problem3.billing where charge>10”);
sqlContext.setConf(“spark.sql.parquet.compression.codec”,”gzip”);
Data.write.parquet(“/user/cert/problem3/colution”)


4.

 

Val data =sqlContext.read.json(“/user/cert/problem4/data/customer”)
Import com.databricks.spark.avro._;
sqlContext.setConf(“spark.sql.avro.compression.codec”,”snappy”)
data.write.avro(“/user/cert/problem4/solution”)


5.

 


Val data=sc.textFile(“/user/cert/problem5/data/customer”)
Val mdata=data.map(p=>(p.split(“,”)(4), p.split(“,”)(5))).toDF(”city”,”state”)
mdata.registerTempTable(“customer”)

Val res=sqlContext.sql(“select city,state,count() from customer group by city,state”)
res.map(p=>p(0)+”,”+p(1)+”,”+p(2)).saveAsTextFile(“”/user/cert/problem5/solution”)

6.

 


Import com.databricks.spark.avro._;
Val data=sqlContext.read.avro(“/user/cert/problem6/data/customer”)
data.registerTempTable(“customer”)

val res=sqlContext.sql(“select select concat(first,’ ’,last) from customer where state=’TX’”)
res.map(p=>p(0)).saveAsTextFile(“/user/cert/problem6/solution/”)

or res.write.text("path") ---we can use write.text method if we have only one column.

7.

 


 


val cust=sc.textFile("/user/cert/problem7/data/customer")
val cdf=cust.map(p=>(p.split("\t")(0),p.split("\t")(1),p.split("\t")(2))).toDF("cid","fname","lname")
val bill=sc.textFile("/user/cert/problem7/data/billing")
val bdf=bill.map(p=>(p.split("\t")(0),p.split("\t")(1))).toDF("bid","amount")
val jdf=cdf.join(bdf,cdf("cid")==bdf("bid")).select("fname","lname","amount")
jdf.map(p=>p(0)+" "+p(1)+"\t"+p(2)).saveAsTextFile("/user/cert/problem7/solution")

8.
 

Val data=sqlContext.read.parquet(“/user/cert/problem8/data/customer”)
data.registerTempTable(“customer”)
val res=sqlContext.sql(“select lname, fname from customer order by lname”)
sqlContext.setConf(“spark.sql.parquet.compression.codec”,”uncompressed”)
res.write.parquet(“/user/cert/problem8/solution”)


-----------------http://arun-teaches-u-tech.blogspot.com/p/certification-preparation-plan.html------------------

prob1:

sqoop import --connect jdbc:mysql://quickstart/retail_db --username root --password cloudera \
--table orders --target-dir /user/cloudera/problem1/orders -z --compression-codec snappy --as-avrodatafile

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table order_items \
--target-dir /user/cloudera/problem1/order_items \
-z \
--compression-codec snappy \
--as-avrodatafile 

import com.databricks.spark.avro._;
val odf=sqlContext.read.avro("/user/cloudera/problem1/orders");
val oidf=sqlContext.read.avro("/user/cloudera/problem1/order_items");

val joindf=odf.join(oidf,odf("order_id")===oidf("order_item_order_id")).select("order_date","order_status","order_id","order_item_subtotal");
order_date: bigint, order_status: string, order_id: int, order_item_subtotal: float

Que:	Order_Date , Order_status, total_orders, total_amount. In plain english, please find total orders and total amount per status per day. 
	The result should be sorted by order date in descending, order status in ascending and 
	total amount in descending and total orders in ascending. Aggregation should be done using below methods. 
	However, sorting can be done using a dataframe or RDD
	
joindf.registerTempTable("join_orders")
val res=sqlContext.sql(" select to_date(from_unixtime(order_date/1000)) as order_date,order_status,count(distinct(order_id)) as total_orders,round(sum(order_item_subtotal),2) as total_amount from join_orders group  by to_date(from_unixtime(order_date/1000)) ,order_status order by order_date desc,order_status,total_amount desc,total_orders");

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip");

res.write.parquet("/user/cloudera/problem1/result-gzip"); //writes into many files to avoid use coalesce(2)
res.coalesce(2).write.parquet("/user/cloudera/problem1/result-gzip");

---validate the results---

sqlContext.read.parquet("/user/cloudera/problem1/result-gzip").show

sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");

res.coalesce(2).write.parquet("/user/cloudera/problem1/result-snappy");

---validate the results---

sqlContext.read.parquet("/user/cloudera/problem1/result-snappy").show

res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(2,true).saveAsTextFile("/user/cloudera/problem1/result-csv")


res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(2,true).saveAsTextFile("/user/cloudera/problem1/result-csv_snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

do cat to read the file data

-------sequence,avro,orc,json--------------

sequence:
---research as below command is not working
res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(2,true).saveAsSequenceFile("/user/cloudera/problem1/result-seq_snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

avro:date is not supported in avro try in another problems

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compress.codec","snappy");
res.coalesce(2).write.avro("/user/cloudera/problem1/result-avro_snappy")

sqlContext.setConf("spark.sql.avro.compress.codec","gzip")
res.coalesce(2).write.avro("/user/cloudera/problem1/result-avro_gzip")



orc:
res.coalesce(2).write.orc("/user/cloudera/problem1/result_orc")

sqlContext.setConf("spark.sql.orc.compress.codec","snappy");
res.coalesce(2).write.orc("/user/cloudera/problem1/result_orc_snappy")

verify using bewlo
sqlContext.read.orc("/user/cloudera/problem1/result_orc").show
sqlContext.read.orc("/user/cloudera/problem1/result_orc_snappy").show



json:

sqlContext.setConf("spark.sql.json.compress.codec","snappy");
res.coalesce(2).write.json("/user/cloudera/problem1/result_json_snappy")

sqlContext.read.json("/user/cloudera/problem1/result_json_snappy").show




----------------


mysql -u root -p 
cloudera

use retail_db;
create table prb1_export(order_date varchar(30),order_status varchar(30),total_orders int,total_amount float);
create table prb1_export_test(order_date varchar(30),order_status varchar(30),total_amount float,total_orders int);


export:
order_date|   order_status|total_orders|total_amount|

sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table prb1_export \
--export-dir /user/cloudera/problem1/result-csv


sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table prb1_export_test \
--export-dir /user/cloudera/problem1/result-csv \
--columns "order_date,order_status,total_orders,total_amount"



res.map(p=>p(0)+"\t"+p(1)+"\t"+p(2)+"\t"+p(3)).coalesce(2,true).saveAsTextFile("/user/cloudera/problem1/result-csvtab")

sqoop export --connect jdbc:mysql://quickstart/retail_db --username root --password cloudera --table prb1_export_tab --export-dir /user/cloudera/problem1/result-csvtab --input-fields-terminated-by '\t'




prob2:

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table products \
--target-dir /user/cloudera/products \
--fields-terminated-by "|"

hdfs dfs -mkdir /user/cloudera/problem2 


hdfs dfs -mv /user/cloudera/products /user/cloudera/problem2

val prdd=sc.textFile("/user/cloudera/problem2/products")
prdd.first.split('|')
val pdf=prdd.map(p=>(p.split('|')(0).toInt,p.split('|')(1).toInt,p.split('|')(4).toFloat)).toDF("pid","cid","price")
pdf.filter(pdf("price")<100).registerTempTable("products");

val res=sqlContext.sql("select cid,round(max(price),2) as h_price,count(distinct(pid)) as pds,round(avg(price),2) as a_price,round(min(price),2) as m_price from products group by cid order by cid desc")

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
res.coalesce(2).write.avro("/user/cloudera/problem2/result-sql")

--verify
sqlContext.read.avro("/user/cloudera/problem2/result-sql").show

---test--
res.map(p=>((p(0)+":"+(p1))+":"+p(2))).first


prob3:

sqoop import-all-tables \
--connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse/retail_stage.db \
-z --compression-codec snappy \
--as-avrodatafile 


order_id|   order_date|order_customer_id|   order_status
create external table orders_sqoop(order_id int,order_date bigint,order_customer_id int,order_status string)
stored as avro
location '/user/hive/warehouse/retail_stage.db/orders'





select y.order_id,to_date(from_unixtime(cast((y.order_date/1000) as bigint))) as order_date,y.order_customer_id,y.order_status from orders_sqoop y
where
y.order_date in (select x.dt from 
(select order_date as dt,count(distinct(order_id)) as order_count from orders_sqoop 
group by order_date order by order_count desc limit 1)  x);


create  table orders_avro(order_id int,order_date bigint,order_customer_id int,order_status string) partitioned by (order_month string)
stored as avro

insert into table orders_avro partition(order_month)
select *,substring(from_unixtime(cast((order_date/1000) as bigint)),1,7)
from orders_sqoop

prob4:

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/text \
--fields-terminated-by '\t' \
--as-textfile


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/avro \
--as-avrodatafile

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/parquet \
--as-parquetfile

import com.databricks.spark.avro._;
val df=sqlContext.read.avro("/user/cloudera/problem5/avro");
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");
df.coalesce(1).write.parquet("/user/cloudera/problem5/parquet-snappy-compress")


df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsTextFile("/user/cloudera/problem5/text-gzip-compress",classOf[org.apache.hadoop.io.compress.GzipCodec])

df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsSequenceFile("/user/cloudera/problem5/sequence")//value saveAsSequenceFile is not a member of org.apache.spark.rdd.RDD[String]

df.map(p=>(p(0).toString,p(0)+","+p(1)+","+p(2)+","+p(3))).coalesce(1,true).saveAsSequenceFile("/user/cloudera/problem5/sequence")



df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsTextFile("/user/cloudera/problem5/text-snappy-compress",classOf[org.apache.hadoop.io.compress.SnappyCodec])



val pdf=sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
pdf.write.parquet("/user/cloudera/problem5/parquet-no-compress");

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
pdf.write.avro("/user/cloudera/problem5/avro-snappy");


val df=sqlContext.read.avro("/user/cloudera/problem5/avro-snappy");
df.toJSON.saveAsTextFile("/user/cloudera/problem5/json-no-compress");
df.toJSON.saveAsTextFile("/user/cloudera/problem5/json-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec]);


val df=sqlContext.read.json("/user/cloudera/problem5/json-gzip")
df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsTextFile("/user/cloudera/problem5/csv-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])



You can use below method as well for setting the compression codec.
sqlContext.sql("SET spark.sql.parquet.compression.codec=snappy")


prob5:

sqoop

create table test_null(id int,name string,age int)

insert into test_null values(1,NULL,23)

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table test_null \
--target-dir /user/cloudera/test_null \
-m 1 \
--fields-terminated-by '|' \
--null-string 'NA' \
--null-non-string -0 


sqoop job:

sqoop job --create first_job \
-- import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table test_null \
--target-dir /user/cloudera/test_incr \
-m 1 \
--fields-terminated-by '|' \
--null-string 'NA' \
--null-non-string -0 \
--check-column id \
--incremental append \
--last-value 0;

sqoop job --exec first_job

prob6:

sqoop import-all-tables --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse \
-m 1 \
--hive-import 

var hc = new org.apache.spark.sql.hive.HiveContext(sc);

hc.sql("select * from orders limit 5").show //it will give error saying database not found.

problem:: under /usr/lib/spark/conf/ you cant find hive-site.xml so you need to place file or create simulink to the file under /usr/lib/hive/conf/ as below

sudo ln -s /usr/lib/hive/conf/hive-site.xml /usr/lib/spark/conf/hive-site.xml



then try now it will work 
