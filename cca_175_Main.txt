



CREATE TABLE #summary (proider_id bigint, speciality VARCHAR(100))
INSERT INTO #summary (proider_id, speciality)


select '12345','LAc'
union all
select '12345','DC'
union all
select '12345','XYZ'
union all
select '12345','DC'
union all
select '55555','LAc'
union all
select '55555','LAc'
union all
select '55555','LAc'
union all
select '55555','XYZ'
union all
select '456789','F2Y'
union all
select '456789','72Y'







select 
proider_id,abc,case when abc like '%DC|%' then 'DC'
							when abc like '%Lac|%' then 'LAc'
							else substring(abc,1,CHARINDEX('|',abc)-1)  end as sp1,

							case when abc like '%LAc|%' and abc  like '%DC|%' then 'LAc'
								else 
								case when (CHARINDEX('|',abc,(charindex('|',abc)+1)))=0 then substring(abc,CHARINDEX('|',abc)+1,10)
									else substring(abc,CHARINDEX('|',abc)+1,(CHARINDEX('|',abc,(charindex('|',abc)+1))))
									end 
								  end as sp2
into #specl
 from 

		 (select proider_id,abc=STUFF(( select '|'+speciality 

		 from (select distinct proider_id,speciality from #summary) t1 where (t1.proider_id=t2.proider_id) FOR XML PATH('')),1,1,'') 

		 from (select distinct proider_id,speciality from #summary) t2

		 group by proider_id) x


		 select * from #specl


		 select s1.proider_id,sp1,sp2 from 

		 #summary s1 join #specl s2
		 on 
		 s1.proider_id=s2.proider_id



prob1:

sqoop import --connect jdbc:mysql://quickstart/retail_db --username root --password cloudera --table orders --target-dir /user/cloudera/problem1/orders -z --compression-codec snappy --as-avrodatafile

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table order_items \
--target-dir /user/cloudera/problem1/order_items \
-z \
--compression-codec snappy \
--as-avrodatafile 

import com.databricks.spark.avro._;
val odf=sqlContext.read.avro("/user/cloudera/problem1/orders");
val oidf=sqlContext.read.avro("/user/cloudera/problem1/order_items");

val joindf=odf.join(oidf,odf("order_id")===oidf("order_item_order_id")).select("order_date","order_status","order_id","order_item_subtotal");
order_date: bigint, order_status: string, order_id: int, order_item_subtotal: float

Que:	Order_Date , Order_status, total_orders, total_amount. In plain english, please find total orders and total amount per status per day. 
	The result should be sorted by order date in descending, order status in ascending and 
	total amount in descending and total orders in ascending. Aggregation should be done using below methods. 
	However, sorting can be done using a dataframe or RDD
	
joindf.registerTempTable("join_orders")
val res=sqlContext.sql(" select to_date(from_unixtime(order_date/1000)) as order_date,order_status,count(distinct(order_id)) as total_orders,round(sum(order_item_subtotal),2) as total_amount from join_orders group  by to_date(from_unixtime(order_date/1000)) ,order_status order by order_date desc,order_status,total_amount desc,total_orders");

sqlContext.setConf("spark.sql.parquet.compression.codec","gzip");

res.write.parquet("/user/cloudera/problem1/result-gzip"); //writes into many files to avoid use coalesce(2)
res.coalesce(2).write.parquet("/user/cloudera/problem1/result-gzip");

---validate the results---

sqlContext.read.parquet("/user/cloudera/problem1/result-gzip").show

sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");

res.coalesce(2).write.parquet("/user/cloudera/problem1/result-snappy");

---validate the results---

sqlContext.read.parquet("/user/cloudera/problem1/result-snappy").show

res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(2,true).saveAsTextFile("/user/cloudera/problem1/result-csv")


res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(2,true).saveAsTextFile("/user/cloudera/problem1/result-csv_snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

do cat to read the file data

-------sequence,avro,orc,json--------------

sequence:
---research as below command is not working
res.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(2,true).saveAsSequenceFile("/user/cloudera/problem1/result-seq_snappy",classOf[org.apache.hadoop.io.compress.SnappyCodec])

avro:date is not supported in avro try in another problems

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compress.codec","snappy");
res.coalesce(2).write.avro("/user/cloudera/problem1/result-avro_snappy")

sqlContext.setConf("spark.sql.avro.compress.codec","gzip")
res.coalesce(2).write.avro("/user/cloudera/problem1/result-avro_gzip")



orc:
res.coalesce(2).write.orc("/user/cloudera/problem1/result_orc")

sqlContext.setConf("spark.sql.orc.compress.codec","snappy");
res.coalesce(2).write.orc("/user/cloudera/problem1/result_orc_snappy")

verify using bewlo
sqlContext.read.orc("/user/cloudera/problem1/result_orc").show
sqlContext.read.orc("/user/cloudera/problem1/result_orc_snappy").show



json:

sqlContext.setConf("spark.sql.json.compress.codec","snappy");
res.coalesce(2).write.json("/user/cloudera/problem1/result_json_snappy")

sqlContext.read.json("/user/cloudera/problem1/result_json_snappy").show




----------------


mysql -u root -p 
cloudera

use retail_db;
create table prb1_export(order_date varchar(30),order_status varchar(30),total_orders int,total_amount float);
create table prb1_export_test(order_date varchar(30),order_status varchar(30),total_amount float,total_orders int);


export:
order_date|   order_status|total_orders|total_amount|

sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table prb1_export \
--export-dir /user/cloudera/problem1/result-csv


sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table prb1_export_test \
--export-dir /user/cloudera/problem1/result-csv \
--columns "order_date,order_status,total_orders,total_amount"



res.map(p=>p(0)+"\t"+p(1)+"\t"+p(2)+"\t"+p(3)).coalesce(2,true).saveAsTextFile("/user/cloudera/problem1/result-csvtab")

sqoop export --connect jdbc:mysql://quickstart/retail_db --username root --password cloudera --table prb1_export_tab --export-dir /user/cloudera/problem1/result-csvtab --input-fields-terminated-by '\t'




prob2:

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table products \
--target-dir /user/cloudera/products \
--fields-terminated-by "|"

hdfs dfs -mkdir /user/cloudera/problem2 


hdfs dfs -mv /user/cloudera/products /user/cloudera/problem2

val prdd=sc.textFile("/user/cloudera/problem2/products")
prdd.first.split('|')
val pdf=prdd.map(p=>(p.split('|')(0).toInt,p.split('|')(1).toInt,p.split('|')(4).toFloat)).toDF("pid","cid","price")
pdf.filter(pdf("price")<100).registerTempTable("products");

val res=sqlContext.sql("select cid,round(max(price),2) as h_price,count(distinct(pid)) as pds,round(avg(price),2) as a_price,round(min(price),2) as m_price from products group by cid order by cid desc")

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
res.coalesce(2).write.avro("/user/cloudera/problem2/result-sql")

--verify
sqlContext.read.avro("/user/cloudera/problem2/result-sql").show

---test--
res.map(p=>((p(0)+":"+(p1))+":"+p(2))).first


prob3:

sqoop import-all-tables \
--connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse/retail_stage.db \
-z --compression-codec snappy \
--as-avrodatafile 


order_id|   order_date|order_customer_id|   order_status
create external table orders_sqoop(order_id int,order_date bigint,order_customer_id int,order_status string)
stored as avro
location '/user/hive/warehouse/retail_stage.db/orders'





select y.order_id,to_date(from_unixtime(cast((y.order_date/1000) as bigint))) as order_date,y.order_customer_id,y.order_status from orders_sqoop y
where
y.order_date in (select x.dt from 
(select order_date as dt,count(distinct(order_id)) as order_count from orders_sqoop 
group by order_date order by order_count desc limit 1)  x);


create  table orders_avro(order_id int,order_date bigint,order_customer_id int,order_status string) partitioned by (order_month string)
stored as avro

insert into table orders_avro partition(order_month)
select *,substring(from_unixtime(cast((order_date/1000) as bigint)),1,7)
from orders_sqoop

prob4:

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/text \
--fields-terminated-by '\t' \
--as-textfile


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/avro \
--as-avrodatafile

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table orders \
--target-dir /user/cloudera/problem5/parquet \
--as-parquetfile

import com.databricks.spark.avro._;
val df=sqlContext.read.avro("/user/cloudera/problem5/avro");
sqlContext.setConf("spark.sql.parquet.compression.codec","snappy");
df.coalesce(1).write.parquet("/user/cloudera/problem5/parquet-snappy-compress")


df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsTextFile("/user/cloudera/problem5/text-gzip-compress",classOf[org.apache.hadoop.io.compress.GzipCodec])

df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsSequenceFile("/user/cloudera/problem5/sequence")//value saveAsSequenceFile is not a member of org.apache.spark.rdd.RDD[String]

df.map(p=>(p(0).toString,p(0)+","+p(1)+","+p(2)+","+p(3))).coalesce(1,true).saveAsSequenceFile("/user/cloudera/problem5/sequence")



df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsTextFile("/user/cloudera/problem5/text-snappy-compress",classOf[org.apache.hadoop.io.compress.SnappyCodec])



val pdf=sqlContext.read.parquet("/user/cloudera/problem5/parquet-snappy-compress")
sqlContext.setConf("spark.sql.parquet.compression.codec","uncompressed");
pdf.write.parquet("/user/cloudera/problem5/parquet-no-compress");

import com.databricks.spark.avro._;
sqlContext.setConf("spark.sql.avro.compression.codec","snappy");
pdf.write.avro("/user/cloudera/problem5/avro-snappy");


val df=sqlContext.read.avro("/user/cloudera/problem5/avro-snappy");
df.toJSON.saveAsTextFile("/user/cloudera/problem5/json-no-compress");
df.toJSON.saveAsTextFile("/user/cloudera/problem5/json-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec]);


val df=sqlContext.read.json("/user/cloudera/problem5/json-gzip")
df.map(p=>p(0)+","+p(1)+","+p(2)+","+p(3)).coalesce(1,true).saveAsTextFile("/user/cloudera/problem5/csv-gzip",classOf[org.apache.hadoop.io.compress.GzipCodec])



You can use below method as well for setting the compression codec.
sqlContext.sql("SET spark.sql.parquet.compression.codec=snappy")


prob5:

sqoop

create table test_null(id int,name string,age int)

insert into test_null values(1,NULL,23)

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table test_null \
--target-dir /user/cloudera/test_null \
-m 1 \
--fields-terminated-by '|' \
--null-string 'NA' \
--null-non-string -0 


sqoop job:

sqoop job --create first_job \
-- import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table test_null \
--target-dir /user/cloudera/test_incr \
-m 1 \
--fields-terminated-by '|' \
--null-string 'NA' \
--null-non-string -0 \
--check-column id \
--incremental append \
--last-value 0;

sqoop job --exec first_job

prob6:

sqoop import-all-tables --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--warehouse-dir /user/hive/warehouse \
-m 1 \
--hive-import 

var hc = new org.apache.spark.sql.hive.HiveContext(sc);

hc.sql("select * from orders limit 5").show //it will give error saying database not found.

problem:: under /usr/lib/spark/conf/ you cant find hive-site.xml so you need to place file or create simulink to the file under /usr/lib/hive/conf/ as below

sudo ln -s /usr/lib/hive/conf/hive-site.xml /usr/lib/spark/conf/hive-site.xml



then try now it will work 


----------------practice---------------


----
product.csv
productID,productCode,name,quantity,price
1001,PEN,Pen Red,5000,1.23
1002,PEN,Pen Blue,8000,1.25
1003,PEN,Pen Black,2000,1.25
1004,PEC,Pencil 2B,10000,0.48
1005,PEC,Pencil 2H,8000,0.49
1006,PEC,Pencil HB,0,9999.99
Now accomplish following activities.
1. Create a Hive ORC table using SparkSql
2. Load this data in Hive table.
3. Create a Hive parquet table using SparkSQL and load data in it.


val pd=sc.textFile("/user/cloudera/practice");
val f=pd.first;
val pdata=pd.filter(p=>p!=f);
val df=pdata.map(p=>(p.split(",")(0).toInt,p.split(",")(1).toString,p.split(",")(2).toString,p.split(",")(3).toInt,p.split(",")(4).toFloat)).toDF("product_id","code","name","quantity","price");
df.write.orc("/user/hive/warehouse/prd_orc_table");
df.write.parquet("/user/hive/warehouse/prd_par_table");

in hive:

create external table prd_orc_table(product_id int,code string,name string,quantity int,price float) stored as orc  location '/user/hive/warehouse/prd_orc_table' 

create external table prd_par_table(product_id int,code string,name string,quantity int,price float) stored as parquet  location '/user/hive/warehouse/prd_par_table' 


------

user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Now accomplish following activities.
1. Import departments table from mysql to hdfs as textfile in departments_text directory.
2. Import departments table from mysql to hdfs as sequncefile in departments_sequence
directory.
3. Import departments table from mysql to hdfs as avro file in departments avro directory.
4. Import departments table from mysql to hdfs as parquet file in departments_parquet
directory.


sqoop import --connect jdbc:mysql://quickstart/retail_db --username retail_dba --password cloudera --table departments --target-dir /user/cloudera/departments_text -m 1 --as-textfile

sqoop import --connect jdbc:mysql://quickstart/retail_db --username retail_dba --password cloudera --table departments --target-dir /user/cloudera/departments_sequence -m 1 --as-sequencefile
sqoop import --connect jdbc:mysql://quickstart/retail_db --username retail_dba --password cloudera --table departments --target-dir /user/cloudera/departments_avro -m 1 --as-avrodatafile

sqoop import --connect jdbc:mysql://quickstart/retail_db --username retail_dba --password cloudera --table departments --target-dir /user/cloudera/departments_parquet -m 1 --as-parquetfile

----

val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2) val b =
a.keyBy(_.length)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, Seq[String])] = Array((4,ArrayBuffer(lion)), (6,ArrayBuffer(spider)),
(3,ArrayBuffer(dog, cat)), (5,ArrayBuffer(tiger, eagle}}}


val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2);
val res=a.map(p=>(p.toString.length,p)).groupByKey
res.collect

--
Problem Scenario 53 : You have been given below code snippet.
val a = sc.parallelize(1 to 10, 3)
operation1
b.collect
Output 1
Array[lnt] = Array(2, 4, 6, 8,10)
operation2
Output 2
Array[lnt] = Array(1,2, 3)
Write a correct code snippet for operation1 and operation2 which will produce desired
output, shown above.


val a = sc.parallelize(1 to 10, 3)
val b=a.filter(a=>(a%2==0))
b.collect
val c=a.filter(a=>(a%2!=0))
c.collect

---

Problem Scenario 65 : You have been given below code snippet.
val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2)
val b = sc.parallelize(1 to a.count.tolnt, 2)
val c = a.zip(b)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(String, Int)] = Array((owl,3), (gnu,4), (dog,1), (cat,2>, (ant,5))



val a = sc.parallelize(List("dog", "cat", "owl", "gnu", "ant"), 2);
val l=a.toArray;
val res=a.map(p=>(p,l.indexOf(p)+1));
res.collect;

--

You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.categories
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. Connect MySQL DB and check the content of the tables.
2. Copy "retaildb.categories" table to hdfs, without specifying directory name.
3. Copy "retaildb.categories" table to hdfs, in a directory name "categories_target".
4. Copy "retaildb.categories" table to hdfs, in a warehouse directory name "categories_warehouse".


sqoop import --connect jdbc:mysql://quickstart/retail_db --username retail_dba --password cloudera --table categories

---
Problem Scenario 63 : You have been given below code snippet.
val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2)
val b = a.map(x => (x.length, x))
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String}] = Array((4,lion), (3,dogcat), (7,panther), (5,tigereagle))


val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "panther", "eagle"), 2);
val b = a.map(x => (x.length, x))
b.reduceByKey((x,y)=>x+y).collect


----

Problem Scenario 89 : You have been given below patient data in csv format,
patientID,name,dateOfBirth,lastVisitDate
1001,Ah Teck,1991-12-31,2012-01-20
1002,Kumar,2011-10-29,2012-09-20
1003,Ali,2011-01-30,2012-10-21
Accomplish following activities.
1. Find all the patients whose lastVisitDate between current time and '2012-09-15'
2. Find all the patients who born in 2011
3. Find all the patients age
4. List patients whose last visited more than 60 days ago
5. Select patients 18 years old or younger


val rd=sc.textFile("/user/cloudera/practice");
val f=rd.first;
val data=rd.filter(p=>p!=f);
val df=data.map(a=>(a.split(",")(0).toInt,a.split(",")(1),a.split(",")(2),a.split(",")(3))).toDF("pid","name","dob","vdate");
df.registerTempTable("patients")

val res1=sqlContext.sql("select * from patients where to_date(vdate)>'2012-09-15'")
res1.show

val res2=sqlContext.sql("select * from patients where dob like '2011%'")


val res3=sqlContext.sql("select *, datediff(to_date(from_unixtime(unix_timestamp())),dob)/365 as age from patients")


val res4=sqlContext.sql("select *  from patients where datediff(to_date(from_unixtime(unix_timestamp())),vdate)>60")

val res5=sqlContext.sql("select *, datediff(to_date(from_unixtime(unix_timestamp())),dob)/365 as age from patients where (datediff(to_date(from_unixtime(unix_timestamp())),dob)/365)<18")

-----


Problem Scenario 48 : You have been given below Python code snippet, with intermediate
output.
We want to take a list of records about people and then we want to sum up their ages and
count them.
So for this example the type in the RDD will be a Dictionary in the format of {name: NAME,
age:AGE, gender:GENDER}.
The result type will be a tuple that looks like so (Sum of Ages, Count)
people = []
people.append({'name':'Amit', 'age':45,'gender':'M'})
people.append({'name':'Ganga', 'age':43,'gender':'F'})
people.append({'name':'John', 'age':28,'gender':'M'})
people.append({'name':'Lolita', 'age':33,'gender':'F'})
people.append({'name':'Dont Know', 'age':18,'gender':'T'})
peopleRdd=sc.parallelize(people) //Create an RDD
peopleRdd.aggregate((0,0), seqOp, combOp) //Output of above line : 167, 5)
Now define two operation seqOp and combOp , such that
seqOp : Sum the age of all people as well count them, in each partition. combOp :
Combine results from all partitions.


Not Solved

----


Problem Scenario 15 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
1. In mysql departments table please insert following record. Insert into departments
values(9999, '"Data Science"1);
2. Now there is a downstream system which will process dumps of this file. However,
system is designed the way that it can process only files if fields are enlcosed in(') single
quote and separate of the field should be (-} and line needs to be terminated by : (colon).
3. If data itself contains the " (double quote } than it should be escaped by \.
4. Please import the departments table in a directory called departments_enclosedby and
file should be able to process by downstream system.


create table departments_rep as select * from departments;

insert into departments_rep values(9999, '"Data Science"');

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments_rep \
--fields-terminated-by '-' \
--lines-terminated-by ':' \
--optionally-enclosed-by '\"' \
--escaped-by '\\' \
--target-dir /user/cloudera/departments_rep \
-m 1


---

Problem Scenario 12 : You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Create a table in retailedb with following definition.
CREATE table departments_new (department_id int(11), department_name varchar(45),
created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);
2. Now isert records from departments table to departments_new
3. Now import data from departments_new table to hdfs.
4. Insert following 5 records in departmentsnew table. Insert into departments_new
values(110, "Civil" , null); Insert into departments_new values(111, "Mechanical" , null);
Insert into departments_new values(112, "Automobile" , null); Insert into departments_new
values(113, "Pharma" , null);
Insert into departments_new values(114, "Social Engineering" , null);
5. Now do the incremental import based on created_date column.


CREATE table departments_new (department_id int(11), department_name varchar(45),
created_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP);

insert into departments_new select * from departments;

sqoop job --create sec_job \
-- import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments_new \
--target-dir /user/cloudera/departments_auto \
--incremental lastmodified \
--check-column created_date \
--merge-key department_id \
-m 1 



sqoop job --exec sec_job
--incremental append will blindly add the data without checking the duplicates(primary key exists) ,if we use --incremental lastmodified then we shoud specify the --merge-key option(only one case where reducer will run)
--last-value '2018-08-26 09:28:43.0'-----this is not required in the while creating job first time



-----

spark3/sparkdir1/file1.txt
spark3/sparkd ir2ffile2.txt
spark3/sparkd ir3Zfile3.txt
Each file contain some text.
spark3/sparkdir1/file1.txt
Apache Hadoop is an open-source software framework written in Java for distributed
storage and distributed processing of very large data sets on computer clusters built from
commodity hardware. All the modules in Hadoop are designed with a fundamental
assumption that hardware failures are common and should be automatically handled by the
framework
spark3/sparkdir2/file2.txt
The core of Apache Hadoop consists of a storage part known as Hadoop Distributed File
System (HDFS) and a processing part called MapReduce. Hadoop splits files into large
blocks and distributes them across nodes in a cluster. To process data, Hadoop transfers
packaged code for nodes to process in parallel based on the data that needs to be
processed.
spark3/sparkdir3/file3.txt
his approach takes advantage of data locality nodes manipulating the data they have
access to to allow the dataset to be processed faster and more efficiently than it would be
in a more conventional supercomputer architecture that relies on a parallel file system
where computation and data are distributed via high-speed networking
Now write a Spark code in scala which will load all these three files from hdfs and do the
word count by filtering following words. And result should be sorted by word count in
reverse order.
Filter words ("a","the","an", "as", "a","with","this","these","is","are","in", "for",
"to","and","The","of")
Also please make sure you load all three files as a Single RDD (All three files must be
loaded using single API call).
You have also been given following codec
import org.apache.hadoop.io.compress.GzipCodec
Please use above codec to compress file, while saving in hdfs.


val rdd=sc.textFile("/user/cloudera/muf/file1,/user/cloudera/muf/file2,/user/cloudera/muf/file3");
val removeRDD = sc.parallelize(List("a","the","an", "as", "a","with","this","these","is","are","in", "for","to","and","The","of"));

val flatrdd=rdd.flatMap(p=>(p.split(" ")));
val trimrdd=flatrdd.map(p=>p.trim);
val filrdd=flatrdd.subtract(removeRDD);
val maprdd=filrdd.map(p=>(p,1)).reduceByKey((x,y)=>x+y).map(p=>p.swap);
val res=maprdd.sortByKey(false);
res.map(p=>p._1+"\t"+p._2).coalesce(1,true).saveAsTextFile("/user/cloudera/wcrev");
res.map(p=>p._1+"\t"+p._2).coalesce(1,true).saveAsTextFile("/user/cloudera/wcrev-zip",classOf[org.apache.hadoop.io.compress.GzipCodec])



-----

user=retail_dba
password=cloudera
database=retail_db
table=retail_db.products

jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of products table : (product_id | product categoryid | product_name |
product_description | product_prtce | product_image )
Please accomplish following activities.
1. Copy "retaildb.products" table to hdfs in a directory p93_products
2. Filter out all the empty prices
3. Sort all the products based on price in both ascending as well as descending order.
4. Sort all the products based on price as well as product_id in descending order.
5. Use the below functions to do data ordering or ranking and fetch top 10 elements top()
takeOrdered() sortByKey()

sqoop command

val productsRDD = sc.textFile("p93_products")

val nonemptylines = productsRDD.filter(lambda x: len(x.split(",")[4]) > 0).map(split and conver the data typs).toDF(col names seperated by ,)

nonemptylines.registerTempTable("pds")

select * from pds order by price desc

select * from pds order by price asc

select * from pds order by pid desc


--15

user=retail_dba
password=cloudera
database=retail_db
table=retail_db.products
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Columns of products table : (product_id | product_category_id | product_name |
product_description | product_price | product_image )
Please accomplish following activities.
1. Copy "retaildb.products" table to hdfs in a directory p93_products
2. Now sort the products data sorted by product price per category, use productcategoryid
colunm to group by category


same as above

select * from pds group by pcid order by price 




----
You have been given following mysql database details as well as
other info.
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Create a database named hadoopexam and then create a table named departments in
it, with following fields. department_id int,
department_name string
e.g. location should be
hdfs://quickstart.cloudera:8020/user/hive/warehouse/hadoopexam.db/departments
2. Please import data in existing table created above from retaidb.departments into hive
table hadoopexam.departments.
3. Please import data in a non-existing table, means while importing create hive table
named hadoopexam.departments_new


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments \
--hive-import \
--hive-table hadoopexam.departments \
-m 1




sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments \
--hive-import \
--hive-table hadoopexam.departments_new \
--create-hive-table \
-m 1


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments \
--hive-import \
--hive-overwrite \
--hive-table hadoopexam.departments_new \
-m 1

--


oozie revisit


---

Problem Scenario 95 : You have to run your Spark application on yarn with each executor
Maximum heap size to be 512MB and Number of processor cores to allocate on each
executor will be 1 and Your main application required three values as input arguments V1
V2 V3.
Please replace XXX, YYY, ZZZ
./bin/spark-submit -class com.hadoopexam.MyTask --master yarn-cluster--num-executors 3
--driver-memory 512m XXX YYY lib/hadoopexam.jarZZZ



sol:

XXX: -executor-memory 512m YYY: -executor-cores 1 ZZZ : V1 V2 V3


---

Problem Scenario 46 : You have been given belwo list in scala (name,sex,cost) for each
work done.
List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female",
2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))
Now write a Spark program to load this list as an RDD and do the sum of cost for
combination of name and sex (as key)


val l=List( ("Deeapak" , "male", 4000), ("Deepak" , "male", 2000), ("Deepika" , "female",
2000),("Deepak" , "female", 2000), ("Deepak" , "male", 1000) , ("Neeta" , "female", 2000))
val rdd=sc.parallelize(l)

val df=rdd.toDF("name","sex","cost")



---

Problem Scenario 41 : You have been given below code snippet.
val aul = sc.parallelize(List (("a" , Array(1,2)), ("b" , Array(1,2))))
val au2 = sc.parallelize(List (("a" , Array(3)), ("b" , Array(2))))
Apply the Spark method, which will generate below output.
Array[(String, Array[lnt])] = Array((a,Array(1, 2)), (b,Array(1, 2)), (a(Array(3)), (b,Array(2)))


au1.union(au2)


--

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3})
val b = a.keyBy(_.length)
val c = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","woif","bear","bee"), 3)
val d = c.keyBy(_.length)
operation1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, (String, String))] = Array((6,(salmon,salmon)), (6,(salmon,rabbit)),
(6,(salmon,turkey)), (6,(salmon,salmon)), (6,(salmon,rabbit)),
(6,(salmon,turkey)), (3,(dog,dog)), (3,(dog,cat)), (3,(dog,gnu)), (3,(dog,bee)), (3,(rat,dog)),
(3,(rat,cat)), (3,(rat,gnu)), (3,(rat,bee)))

ans: b.join(d).collect

---


user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following.
1. Create a table in retailedb with following definition.
CREATE table departments_export (department_id int(11), department_name varchar(45),
created_date T1MESTAMP DEFAULT NOWQ);
2. Now import the data from following directory into departments_export table,
/user/cloudera/departments new




sqoop export --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments_export \
--export-dir /user/cloudera/departments \



-----

Problem Scenario 23 : You have been given log generating service as below.
Start_logs (It will generate continuous logs)
Tail_logs (You can check , what logs are being generated)
Stop_logs (It will stop the log service)
Path where logs are generated using above service : /opt/gen_logs/logs/access.log
Now write a flume configuration file named flume3.conf , using that configuration file dumps
logs in HDFS file system in a directory called flumeflume3/%Y/%m/%d/%H/%M
Means every minute new directory should be created). Please us the interceptors to
provide timestamp information, if message header does not have header info.
And also note that you have to preserve existing timestamp, if message contains it. Flume
channel should have following property as well. After every 100 message it should be
committed, use non-durable/faster channel and it should be able to hold maximum 1000
events.


flume----


---
user=retail_dba
password=cloudera
database=retail_db
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish below assignment.
1. Create a table in hive as below.
create table departments_hive(department_id int, department_name string);
2. Now import data from mysql table departments to this hive table. Please make sure that
data should be visible using below hive command, select" from departments_hive



The important here is, when we create a table without delimiter fields. Then default delimiter for hive is ^A (\001)

sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table departments \
--hive-import \
--hive-table hadoopexam.departments_hive \
--fields-terminated-by '\001'

---

Problem Scenario 92 : You have been given a spark scala application, which is bundled in
jar named hadoopexam.jar.
Your application class name is com.hadoopexam.MyTask
You want that while submitting your application should launch a driver on one of the cluster
node.
Please complete the following command to submit the application.
spark-submit XXX -master yarn \
YYY SSPARK HOME/lib/hadoopexam.jar 10


sol:
spark-submit --class com.hadoopexam.MyTask --master yarn --deploy-mode cluster 


---

Problem Scenario 43 : You have been given following code snippet.
val grouped = sc.parallelize(Seq(((1,"two"), List((3,4), (5,6)))))
val flattened = grouped.flatMap {A =>groupValues.map { value => B }
You need to generate following output.
Hence replace A and B
Array((1,two,3,4),(1,two,5,6))


solution needs to find

----

val a = sc.parallelize(List("dog", "salmon", "salmon", "rat", "elephant"), 3)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("dog","cat","gnu","salmon","rabbit","turkey","wolf","bear","bee"), 3)
val d = c.keyBy(_.length) operationl
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, (String, Option[String]}}] = Array((6,(salmon,Some(salmon))),
(6,(salmon,Some(rabbit))),
(6,(salmon,Some(turkey))), (6,(salmon,Some(salmon))), (6,(salmon,Some(rabbit))),
(6,(salmon,Some(turkey))), (3,(dog,Some(dog))), (3,(dog,Some(cat))),
(3,(dog,Some(dog))), (3,(dog,Some(bee))), (3,(rat,Some(dogg)), (3,(rat,Some(cat)j),
(3,(rat.Some(gnu))). (3,(rat,Some(bee))), (8,(elephant,None)))


b.leftOuterJoin(d).collect



----


Problem Scenario 94 : You have to run your Spark application on yarn with each executor
20GB and number of executors should be 50. Please replace XXX, YYY, ZZZ
export HADOOP_CONF_DIR=XXX
./bin/spark-submit \
-class com.hadoopexam.MyTask \
xxx\
-deploy-mode cluster \ # can be client for client mode
YYY\
zzz \
/path/to/hadoopexam.jar \
1000


xxx:--master yarn 
yyy: --num-executors 50
zzz: --executor-memory 20G



----


 You have been given MySQL DB with following details.
user=retail_dba
password=cloudera
database=retail_db
table=retail_db.categories
jdbc URL = jdbc:mysql://quickstart:3306/retail_db
Please accomplish following activities.
Import Single table categories (Subset data} to hive managed table , where category_id
between 1 and 22


sqoop import --connect jdbc:mysql://quickstart/retail_db \
--username root \
--password cloudera \
--table categories \
--where 'category_id between 1 and 22' \
--target-dir /user/cloudera/cat_where \
--hive-table cat_where \
-m 1

----

spark16/file1.txt
1,9,5
2,7,4
3,8,3
spark16/file2.txt
1,g,h
2,i,j
3,k,l
Load these two tiles as Spark RDD and join them to produce the below results
(l,((9,5),(g,h)))
(2, ((7,4), (i,j))) (3, ((8,3), (k,l)))
And write code snippet which will sum the second columns of above joined results (5+4+3).


sol:

val data1=scala.io.Source.fromFile("/home/cloudera/Desktop/file1").getLines.toList
val data2=scala.io.Source.fromFile("/home/cloudera/Desktop/file2").getLines.toList
val rdd1=sc.parallelize(data1)
val rdd2=sc.parallelize(data2)
val r1map=rdd1.map(p=>(p.split(",")(0).toInt,(p.split(",")(1),p.split(",")(2))))
val r2map=rdd2.map(p=>(p.split(",")(0).toInt,(p.split(",")(1),p.split(",")(2))))
r1map.join(r2map).sortBy(p=>p._1).collect

r1map.join(r2map).sortBy(p=>p._1).map(p=>p._2._1._2.toInt).reduce((x,y)=>x+y)

----


val a = sc.parallelize(List("dog", "tiger", "lion", "cat", "spider", "eagle"), 2)
val b = a.keyBy(_.length)
val c = sc.parallelize(List("ant", "falcon", "squid"), 2)
val d = c.keyBy(.length)
operation 1
Write a correct code snippet for operationl which will produce desired output, shown below.
Array[(lnt, String)] = Array((4,lion))


b.subtractByKey(d).collect


---


